{
 "metadata": {
  "name": "2_2_mlp"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multilayer Perceptron and Convolutional Net\n",
      "===========================================\n",
      "\n",
      "In the previous tutorial on linear SVMs, we looked at how training a model\n",
      "consists in defining a training objective, and then minimizing it with respect\n",
      "to model parameters by either stochastic or batch gradient descent.\n",
      "\n",
      "In this tutorial we will augment our SVM with _internal layers_ of _hidden\n",
      "units_ and turn our linear classifier into a deep (multi-layer) architecture.\n",
      "\n",
      "An MLP can be viewed as an SVM, where the input $\\mathbf{x}$ is first transformed using a\n",
      "learnt non-linear vector-valued transformation $\\Phi$.\n",
      "The purpose of this transformation is to map the input data into a space where it becomes linearly separable.\n",
      "The vector $\\Phi(\\mathbf{x})$ is referred to as a _hidden layer_.\n",
      "\n",
      "This tutorial will again tackle the problem of MNIST digit classification.\n",
      "XXX\n",
      "\n",
      "The Model\n",
      "+++++++++\n",
      "\n",
      "Typically the function $\\Phi$ is taken to be the function\n",
      "$\\mathbb{R}^{D_0} \\rightarrow (-1, 1)^{D_1}$\n",
      "\n",
      "$$\n",
      "\\Phi(\\mathbf{x}; V, \\mathbf{c}) = \\tanh( \\mathbf{x} V + \\mathbf{c})\n",
      "$$\n",
      "\n",
      "in which $V \\in \\mathbb{R}^{D_0 \\times D_1}$ is called a _weight matrix_, and\n",
      "$\\mathbf{c} \\in \\mathbb{R}^{D_1}$ is called a _bias vector_.\n",
      "The integer $D_0$ is the number of elements in $\\mathbf{x}$ (sometimes, number\n",
      "of \"input units\").\n",
      "The integer $D_1$ is the number of \"hidden units\" of the MLP.\n",
      "We abuse notation slightly here by using $\\tanh(\\mathbf{u})$ for a vector\n",
      "$\\mathbf{u}$ to denote\n",
      "the vector of values $\\tanh(\\mathbf{u}_i)$.\n",
      "Sometimes other non-linear scalar functions are used instead of the tanh\n",
      "function -- whichever one is used is called the _activation function_ of\n",
      "layer $\\Phi$.\n",
      "\n",
      "When combined with an SVM classifier (recall previous tutorial), the full classification model can be written\n",
      "\n",
      "$$\n",
      "    \\mathrm{MLP}(\\mathbf{x}) = \\mathrm{SVM}\\left(\\Phi(\\mathbf{x}; V, \\mathbf{c})); W, \\mathbf{b} \\right)\n",
      "$$\n",
      "\n",
      "This sort of MLP (or Artificial Neural Network - ANN) with a single hidden layer\n",
      "is sometimes represented graphically as follows:\n",
      "\n",
      "<img src=\"files/images/mlp.png\" align=center/>\n",
      "\n",
      "A single hidden layer of this form is sufficient to make the MLP a universal approximator.\n",
      "However we will see shortly\n",
      "that there are benefits to using many such hidden layers (deep learning).\n",
      "See these course notes for an\n",
      "[introduction to MLPs, the back-propagation algorithm, and how to train MLPs](http://www.iro.umontreal.ca/~pift6266/H10/notes/mlp.html).\n",
      "\n",
      "\n",
      "## Training an MLP\n",
      "\n",
      "To train an MLP, we learn **all** parameters of the model ($W, \\mathbf{b}, V,\n",
      "\\mathbf{c}$) by gradient descent,\n",
      "just as we learned the parameters $W, \\mathbf{b}$ previously when training the\n",
      "SVM.\n",
      "\n",
      "\n",
      "The initial values for the weights of a hidden layer ($V$) should be uniformly\n",
      "sampled from a symmetric interval that depends on the activation function.\n",
      "For the tanh activation function results obtained in [Xavier10]_ show that the\n",
      "interval should be\n",
      "$$\n",
      "[-\\sqrt{\\frac{6}{D_0 + D_1}}, \\sqrt{\\frac{6}{D_0 + D_1}}]\n",
      "$$\n",
      "\n",
      "For the logistic sigmoid function $1 / (1 + e^{-u})$ the interval is slightly\n",
      "different:\n",
      "\n",
      "$$\n",
      "[-4\\sqrt{\\frac{6}{D_0 + D_1}},4\\sqrt{\\frac{6}{D_0 + D_1}}]\n",
      "$$.\n",
      "\n",
      "This initialization ensures that at least early in training, each neuron operates in a regime of its activation function where information can easily be propagated both upward (activations flowing from inputs to outputs) and backward (gradients flowing from outputs to inputs).\n",
      "\n",
      "### Exercise: Breaking symmetry\n",
      "\n",
      "It might seem more natural to initialize all of the MLP parameters to $0$.\n",
      "What goes wrong when you do this?\n",
      "\n",
      "\n",
      "As before, we train this model using stochastic gradient descent with\n",
      "mini-batches. The difference is that we modify the cost function to include the\n",
      "regularization term. `L1_reg` and `L2_reg` are the hyperparameters\n",
      "controlling the weight of these regularization terms in the total cost function.\n",
      "\n",
      "XXX\n",
      "\n",
      "To put this into perspective, we refer the reader to the results section of\n",
      "the\n",
      "[MNIST Score Board](http://yann.lecun.com/exdb/mnist).\n",
      "\n",
      "\n",
      "## Tips and Tricks for training MLPs\n",
      "\n",
      "There are several hyper-parameters in the above code, which are not (and,\n",
      "generally speaking, cannot be) optimized by gradient descent.\n",
      "The design of outer-loop algorithms for optimizing them is a topic of ongoing\n",
      "research.\n",
      "Over the last 25 years, researchers have devised various rules of thumb for choosing them.\n",
      "A very good overview of these tricks can be found in [Efficient\n",
      "BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) by Yann LeCun,\n",
      "Leon Bottou, Genevieve Orr, and Klaus-Robert Mueller. Here, we summarize\n",
      "the same issues, with an emphasis on the parameters and techniques that we\n",
      "actually used in our code.\n",
      "\n",
      "### Tips and Tricks: Nonlinearity\n",
      "\n",
      "Which non-linear activation function should you use in a neural network?\n",
      "Two of the most common ones are the logistic sigmoid and the tanh functions.\n",
      "For reasons explained in [Section 4.4](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf), nonlinearities that\n",
      "are symmetric around the origin are preferred because they tend to produce\n",
      "zero-mean inputs to the next layer (which is a desirable property).\n",
      "Empirically, we have observed that the tanh has better convergence properties.\n",
      "\n",
      "\n",
      "### Tips and Tricks: Weight initialization\n",
      "\n",
      "At initialization we want the weights to be small enough around the origin\n",
      "so that the activation function operates near its linear regime, where gradients are\n",
      "the largest. Otherwise, the gradient signal used for learning is attenuated by\n",
      "each layer as it is propagated from the classifier towards the inputs.\n",
      "Other desirable properties, especially for deep networks,\n",
      "are to conserve variance of the activation as well as variance of back-propagated gradients from layer to layer.\n",
      "This allows information to flow well upward and downward in the network and\n",
      "reduces discrepancies between layers.\n",
      "The initialization used above represents a good compromise between these two\n",
      "constraints.\n",
      "For mathematical considerations, please refer to [Xavier10]_.\n",
      "\n",
      "\n",
      "### Tips and Tricks: Learning Rate\n",
      "\n",
      "Optimization by stochastic gradient descent is very sensitive to the step size or _learning rate_.\n",
      "There is a great deal of literature on how to choose a the learning rate, and how to change it during optimization.\n",
      "The simplest solution is to use a constant rate. Rule of thumb: try\n",
      "several log-spaced values ($10^{-1}, 10^{-2}, \\ldots$) and narrow the\n",
      "(logarithmic) grid search to the region where you obtain the lowest\n",
      "validation error.\n",
      "\n",
      "Decreasing the learning rate over time can help a model to settle down into a\n",
      "[local] minimum.\n",
      "One simple rule for doing that is $\\frac{\\mu_0}{1 + d\\times t}$ where\n",
      "$\\mu_0$ is the initial rate (chosen, perhaps, using the grid search\n",
      "technique explained above), $d$ is a so-called \"decrease constant\"\n",
      "which controls the rate at which the learning rate decreases (typically, a\n",
      "smaller positive number, $10^{-3}$ and smaller) and $t$ is the epoch/stage.\n",
      "\n",
      "[Section 4.7](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) details\n",
      "procedures for choosing a learning rate for each parameter (weight) in our\n",
      "network and for choosing them adaptively based on the error of the classifier.\n",
      "\n",
      "### Tips and Tricks: Number of hidden units\n",
      "\n",
      "The number of hidden units that gives best results is dataset-dependent.\n",
      "Generally speaking, the more complicated the input distribution is, the more capacity the network\n",
      "will require to model it, and so the larger the number of hidden units thatwill be needed (note that the number of weights in a layer, perhaps a more direct\n",
      "measure of capacity, is $D_0\\times D_1$ (recall $D_0$ is the number of inputs and\n",
      "$D_1$ is the number of hidden units).\n",
      "\n",
      "Unless we employ some regularization scheme (early stopping or L1/L2\n",
      "penalties), a typical number of hidden  units vs. generalization performance graph will be U-shaped.\n",
      "\n",
      "### Tips and Tricks: Norm Regularization\n",
      "\n",
      "Typical values to try for the L1/L2 regularization parameter $\\lambda$ are $10^{-2}, 10^{-3}, \\ldots$.\n",
      "It can be useful to regularize the topmost layers in an MLP (closest\n",
      "to and including the classifier itself) to prevent them from overfitting noisy\n",
      "hidden layer features, and to encourage the features themselves to be more\n",
      "discriminative.\n",
      "\n",
      "\n",
      "\n",
      "# Convolutional Networks\n",
      "========================\n",
      "\n",
      "Convolutional Neural Networks (ConvNets) are MLP variants which are\n",
      "specialized for image processing.\n",
      "From Hubel and Wiesel's early work on the cat's visual cortex [Hubel68]_,\n",
      "we know there exists a complex arrangement of cells within the visual cortex.\n",
      "These cells are sensitive to small sub-regions of the input space, called a\n",
      "_receptive field_, and are tiled in such a way as to cover the entire visual\n",
      "field. These filters are local in input space and are thus better suited to\n",
      "exploit the strong spatially local correlation present in natural images.\n",
      "\n",
      "Additionally, two basic cell types have been identified: simple cells (S) and\n",
      "complex cells (C). Simple cells (S) respond maximally to specific edge-like\n",
      "stimulus patterns within their receptive field. Complex cells (C) have larger\n",
      "receptive fields and are locally invariant to the exact position of the\n",
      "stimulus.\n",
      "\n",
      "\n",
      "## Sparse Connectivity\n",
      "\n",
      "ConvNets (as well as several related computer vision architectures XXX),\n",
      "exploit spatially local correlation by enforcing a local connectivity pattern between\n",
      "neurons of adjacent layers. The input hidden units in the m-th layer are\n",
      "connected to a local subset of units in the (m-1)-th layer, which have spatiallycontiguous receptive fields. We can illustrate this graphically as follows:\n",
      "\n",
      "<img src=\"files/images/sparse_1D_nn.png\", align=center/>\n",
      "\n",
      "Imagine that layer **m-1** is the input retina.\n",
      "In the above, units in layer **m**\n",
      "have receptive fields of width 3 with respect to the input retina and are thus only\n",
      "connected to 3 adjacent neurons in the layer below (the retina).\n",
      "Units in layer **m** have\n",
      "a similar connectivity with the layer below. We say that their receptive\n",
      "field with respect to the layer below is also 3, but their receptive field\n",
      "with respect to the input is larger (it is 5).\n",
      "The architecture thus\n",
      "confines the learnt \"filters\" (corresponding to the input producing the strongest response) to be a spatially local pattern\n",
      "(since each unit is unresponsive to variations outside of its receptive field with respect to the retina).\n",
      "As shown above, stacking many such\n",
      "layers leads to \"filters\" (not anymore linear) which become increasingly \"global\" however (i.e\n",
      "spanning a larger region of pixel space). For example, the unit in hidden\n",
      "layer **m+1** can encode a non-linear feature of width 5 (in terms of pixel\n",
      "space).\n",
      "\n",
      "\n",
      "## Shared Weights\n",
      "\n",
      "In ConvNets, each sparse filter $h_i$ is additionally replicated across the\n",
      "entire visual field. These \"replicated\" units form a **feature map**, which\n",
      "share the same parametrization, i.e. the same weight vector and the same bias.\n",
      "\n",
      "<img src=\"files/images/conv_1D_nn.png\" align=center/>\n",
      "\n",
      "In the above figure, we show 3 hidden units belonging to the same feature map.\n",
      "Weights of the same color are shared, i.e. are constrained to be identical.\n",
      "Gradient descent can still be used to learn such shared parameters, and\n",
      "requires only a small change to the original algorithm. The gradient of a\n",
      "shared weight is simply the sum of the gradients of the parameters being\n",
      "shared.\n",
      "\n",
      "Why are shared weights interesting ? Replicating units in this way allows for\n",
      "features to be detected regardless of their position in the visual field.\n",
      "Additionally, weight sharing offers a very efficient way to do this, since it\n",
      "greatly reduces the number of free parameters to learn. By controlling model\n",
      "capacity, ConvNets tend to achieve better generalization on vision problems.\n",
      "\n",
      "## Details and Notation\n",
      "\n",
      "Conceptually, a feature map is obtained by convolving the input image with a\n",
      "linear filter, adding a bias term and then applying a non-linear function. If\n",
      "we denote the k-th feature map at a given layer as $h^k$, whose filters\n",
      "are determined by the weights $W^k$ and bias $b_k$, then the\n",
      "feature map $h^k$ is obtained as follows (for $tanh$ non-linearities):\n",
      "\n",
      "$$\n",
      "    h^k_{ij} = \\tanh ( (W^k * x)_{ij} + b_k ).\n",
      "$$\n",
      "\n",
      "To form a richer representation of the data, hidden layers are composed of\n",
      "a set of multiple feature maps, $\\{h^{(k)}, k=0..K\\}$.\n",
      "The weights $W$ of this layer can be parametrized as a 4D tensor\n",
      "(destination feature map index, source feature map index, source vertical position index, source horizontal position index)\n",
      "and\n",
      "the biases $b$ as a vector (one element per destination feature map index).\n",
      "We illustrate this graphically as follows:\n",
      "\n",
      "<img src=\"files/images/cnn_explained.png\" align=center />\n",
      "\n",
      "**Figure 1**: example of a convolutional layer\n",
      "\n",
      "Here, we show two layers of a ConvNet, containing 4 feature maps at layer (m-1)\n",
      "and 2 feature maps ($h^0$ and $h^1$) at layer m. Pixels (neuron outputs) in\n",
      "$h^0$ and $h^1$ (outlined as blue and red squares) are computed\n",
      "from pixels of layer (m-1) which fall within their 2x2 receptive field in the\n",
      "layer below (shown\n",
      "as colored rectangles). Notice how the receptive field spans all four input\n",
      "feature maps. The weights $W^0$ and $W^1$ of $h^0$ and\n",
      "$h^1$ are thus 3D weight tensors. The leading dimension indexes the\n",
      "input feature maps, while the other two refer to the pixel coordinates.\n",
      "\n",
      "Putting it all together, $W^{kl}_{ij}$ denotes the weight connecting\n",
      "each pixel of the k-th feature map at layer m, with the pixel at coordinates\n",
      "(i,j) of the l-th feature map of layer (m-1).\n",
      "\n",
      "### Exercise: Normalized Filterbank Cross-Correlation\n",
      "Let's have a little bit of fun with this...\n",
      "\n",
      ".. code-block:: python\n",
      "\n",
      "        import pylab\n",
      "        from PIL import Image\n",
      "\n",
      "        # open random image of dimensions 639x516\n",
      "        img = Image.open(open('images/3wolfmoon.jpg'))\n",
      "        img = numpy.asarray(img, dtype='float64') / 256.\n",
      "\n",
      "        # put image in 4D tensor of shape (1, 3, height, width)\n",
      "        img_ = img.swapaxes(0, 2).swapaxes(1, 2).reshape(1, 3, 639, 516)\n",
      "        filtered_img = f(img_)\n",
      "\n",
      "        # plot original image and first and second components of output\n",
      "        pylab.subplot(1, 3, 1); pylab.axis('off'); pylab.imshow(img)\n",
      "        pylab.gray();\n",
      "        # recall that the convOp output (filtered image) is actually a \"minibatch\",\n",
      "        # of size 1 here, so we take index 0 in the first dimension:\n",
      "        pylab.subplot(1, 3, 2); pylab.axis('off'); pylab.imshow(filtered_img[0, 0, :, :])\n",
      "        pylab.subplot(1, 3, 3); pylab.axis('off'); pylab.imshow(filtered_img[0, 1, :, :])\n",
      "        pylab.show()\n",
      "\n",
      "Notice that a randomly initialized filter acts very much like an edge detector!\n",
      "\n",
      "Also of note, remark that we use the same weight initialization formula as\n",
      "with the MLP. Weights are sampled randomly from a uniform distribution in the\n",
      "range [-1/$D_0$, 1/$D_0$], where $D_0$ is the number of inputs to a hidden\n",
      "unit. For MLPs, this was the number of units in the layer below. For ConvNets\n",
      "however, we have to take into account the number of input feature maps and the\n",
      "size of the receptive fields.\n",
      "\n",
      "## MaxPooling\n",
      "\n",
      "ConvNets emulate the behaviour of \"complex cells\" in visual cortex by\n",
      "_max-pooling_, which is a form of\n",
      "non-linear down-sampling. Max-pooling partitions the input image into\n",
      "a set of non-overlapping rectangles and, for each such sub-region, outputs the\n",
      "maximum value.\n",
      "Max-pooling is useful in vision for two reasons: (1) it reduces the\n",
      "computational complexity for upper layers and (2) it provides a form of\n",
      "translation invariance. To understand the invariance argument, imagine\n",
      "cascading a max-pooling layer with a convolutional layer. There are 8\n",
      "directions in which one can translate the input image by a single pixel. If\n",
      "max-pooling is done over a 2x2 region, 3 out of these 8 possible\n",
      "configurations will produce exactly the same output at the convolutional\n",
      "layer. For max-pooling over a 3x3 window, this jumps to 5/8.\n",
      "\n",
      "Since it provides additional robustness to position, max-pooling is thus a\n",
      "\"smart\" way of reducing the dimensionality of intermediate representations.\n",
      "\n",
      "XXX\n",
      "\n",
      "Max-pooling is done in Theano by way of `theano.tensor.signal.downsample.max_pool_2d`.\n",
      "This function takes as input an N dimensional tensor (with N >= 2), a\n",
      "downscaling factor and performs max-pooling over the 2 trailing dimensions of\n",
      "the tensor.\n",
      "\n",
      "\n",
      "## Stacking Layers into a ConvNet\n",
      "\n",
      "A ConvNet architecture interleaves convolutional and pooling layers to produce\n",
      "the input to an MLP.\n",
      "\n",
      "<img src=\"files/images/mylenet.png\" align=center />\n",
      "\n",
      "From an implementation point of view, this means lower-layers operate on 4D\n",
      "tensors. These are then flattened to a 2D matrix of rasterized feature maps,\n",
      "to be compatible with our previous MLP implementation.\n",
      "\n",
      "\n",
      "## Training a ConvNet\n",
      "\n",
      "\n",
      "## Tips and Tricks\n",
      "\n",
      "### Tips and Tricks: Choosing Hyperparameters\n",
      "\n",
      "ConvNets are especially tricky to train, as they add even more hyper-parameters than\n",
      "a standard MLP. While the usual rules of thumb for learning rates andregularization constants still apply, the following should be kept in mind when\n",
      "optimizing ConvNets.\n",
      "\n",
      "### Tips and Tricks: Number of filters\n",
      "\n",
      "When choosing the number of filters per layer, keep in mind that computing the\n",
      "activations of a single convolutional filter is much more expensive than with\n",
      "traditional MLPs!\n",
      "\n",
      "Assume layer $(l-1)$ contains $K^{l-1}$ feature\n",
      "maps and $M \\times N$ pixel positions (i.e.,\n",
      "number of positions times number of feature maps),\n",
      "and there are $K^l$ filters at layer $l$ of shape $m \\times n$.\n",
      "Then computing a feature map (applying an $m \\times n$ filter\n",
      "at all $(M-m) \\times (N-n)$ pixel positions where the\n",
      "filter can be applied) costs $(M-m) \\times (N-n) \\times m \\times n \\times K^{l-1}$.\n",
      "The total cost is $K^l$ times that. Things may be more complicated if\n",
      "not all features at one level are connected to all features at the previous one.\n",
      "\n",
      "For a standard MLP, the cost would only be $K^l \\times K^{l-1}$\n",
      "where there are $K^l$ different neurons at level $l$.\n",
      "As such, the number of filters used in ConvNets is typically much\n",
      "smaller than the number of hidden units in MLPs and depends on the size of the\n",
      "feature maps (itself a function of input image size and filter shapes).\n",
      "\n",
      "Since feature map size decreases with depth, layers near the input layer will tend to\n",
      "have fewer filters while layers higher up can have much more. In fact, to\n",
      "equalize computation at each layer, the product of the number of features\n",
      "and the number of pixel positions is typically picked to be roughly constant\n",
      "across layers. To preserve the information about the input would require\n",
      "keeping the total number of activations (number of feature maps times\n",
      "number of pixel positions) to be non-decreasing from one layer to the next\n",
      "(of course we could hope to get away with less when we are doing supervised\n",
      "learning). The number of feature maps directly controls capacity and so\n",
      "that depends on the number of available examples and the complexity of \n",
      "the task.\n",
      "\n",
      "\n",
      "### Tips and Tricks: Filter Shape\n",
      "\n",
      "Common filter shapes found in the litterature vary greatly, usually based on\n",
      "the dataset. Best results on MNIST-sized images (28x28) are usually in the 5x5range on the first layer, while natural image datasets (often with hundreds of pixels in each\n",
      "dimension) tend to use larger first-layer filters of shape 12x12 or 15x15.\n",
      "\n",
      "The trick is thus to find the right level of \"granularity\" (i.e. filter\n",
      "shapes) in order to create abstractions at the proper scale, given a\n",
      "particular dataset.\n",
      "\n",
      "\n",
      "### Tips and Tricks: Max Pooling Shape\n",
      "\n",
      "Typical values are 2x2 or no max-pooling. Very large input images may warrant\n",
      "4x4 pooling in the lower-layers. Keep in mind however, that this will reduce the\n",
      "dimension of the signal by a factor of 16, and may result in throwing away too\n",
      "much information.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}