{
 "metadata": {
  "name": "3_2_patch_features"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Learning features from Image Patches\n",
      "\n",
      "Unsupervised learning algorithms have often tackled image modeling by first modeling small image patches.\n",
      "This tutorial walks through the process of learning features from raw image patches, and then introduces a _whitening_ algorithm that is often used to pre-process image patches before learning.\n",
      "Whitening is biologically plausible, gives rise to physiologically normal first layer features, and typically yields better supervised performance.\n",
      "\n",
      "The whitening algorithm introduced here is borrowed from Adam Coates'\n",
      "[matlab sample code](http://www.stanford.edu/~acoates/papers/sc_vq_demo.tgz) accompanying his paper\n",
      "[The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization](http://www.stanford.edu/~acoates/papers/coatesng_icml_2011.pdf) with Andrew Ng at ICML 2011.\n",
      "\n",
      "Whitening is often not discussed in detail in papers, but it can be crucial to reproducing state-of-the-art performance. The [Stanford UFLDL Tutorial](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)\n",
      "has a nice section on whitening.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Learning an AutoEncoder on Raw Patches\n",
      "\n",
      "Building on the previous notebook on autoencoders, let's see how a standard autoencoder works out of the box on image patches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- SCRIPT INITIALIZATION\n",
      "\n",
      "import numpy as np\n",
      "import autodiff\n",
      "from util import show_filters\n",
      "from util import random_patches\n",
      "from util import mean_and_std\n",
      "from skdata import cifar10\n",
      "\n",
      "n_examples = 10000\n",
      "n_patches = 10000\n",
      "patch_H = 8\n",
      "patch_W = 8\n",
      "dtype='float32'\n",
      "# -- TIP: restart the IPython kernel to clear out memory\n",
      "data_view = cifar10.view.OfficialImageClassification(x_dtype=dtype, n_train=n_examples) \n",
      "x = data_view.train.x[:n_examples]\n",
      "x_patches = random_patches(x.transpose(0, 3, 1, 2), n_patches, patch_H, patch_W, np.random)\n",
      "x_patches_flat = x_patches.reshape(n_patches, -1)\n",
      "\n",
      "# -- define a show_filters function for our rgb data\n",
      "def show_flat_rgb(X, grid_shp):\n",
      "    K = patch_W * patch_H\n",
      "    # this is a bit of funny business to show RGB\n",
      "    show_filters((X[:, :K], X[:, K:2 * K], X[:, 2 * K:], None), (patch_H, patch_W), grid_shp) \n",
      "\n",
      "show_flat_rgb(x_patches_flat, (10, 10))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- DEFINE THE AUTO-ENCODER\n",
      "\n",
      "def logistic(u):\n",
      "    \"\"\"Return logistic sigmoid of float or ndarray `u`\"\"\"\n",
      "    return 1.0 / (1.0 + exp(-u))\n",
      "\n",
      "def squared_distance(x, z):\n",
      "    return ((x - z) ** 2).sum(axis=1)\n",
      "\n",
      "\n",
      "def training_criterion(W, b, c, x):\n",
      "    h = logistic(dot(x, W) + b)\n",
      "    z = logistic(dot(h, W.T) + c)\n",
      "    return squared_distance(x, z).mean()\n",
      "\n",
      "# -- tip: choose the number of hidden units as a pair, so that show_filters works\n",
      "n_visible = x_patches_flat.shape[1]\n",
      "n_hidden2 = (20, 20)\n",
      "n_hidden = np.prod(n_hidden2)\n",
      "W = np.random.uniform(\n",
      "        low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        size=(n_visible, n_hidden)).astype(dtype)\n",
      "b = np.zeros(n_hidden).astype(dtype)\n",
      "c = np.zeros(n_visible).astype(dtype)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- TRAIN THE AUTOENCODER ON RAW PATCHES\n",
      "\n",
      "streams={'x': x_patches.reshape((n_examples, 1, n_visible))}\n",
      "W, b, c = autodiff.fmin_sgd(training_criterion,\n",
      "        args=(W, b, c),\n",
      "        streams=streams,\n",
      "        stepsize=0.01,\n",
      "        loops=5,  # -- fmin_sgd can iterate over the streams repeatedly\n",
      "        print_interval=1000,\n",
      "        )\n",
      "show_flat_rgb(W.T, n_hidden2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you will probably see - despite the recontruction error dropping steadily, the filters of our auto-encoder do not resemble anything like Gabor edge detectors.  To see Gabors, we will need to whiten the image patches."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Learning an AutoEncoder on Whitened Patches\n",
      "\n",
      "There are several techniques for whitening, this one combines a technique used in recent papers from Andrew Ng's group at Stanford, with techniques developed by Nicolas Pinto at MIT."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- fun hyper-parameters to optimize. By fun, I admittedly mean annoying.\n",
      "remove_mean = True\n",
      "hard_beta = True\n",
      "beta = 10.0\n",
      "gamma = 0.01\n",
      "\n",
      "def contrast_normalize(patches):\n",
      "    X = patches\n",
      "    if X.ndim != 2:\n",
      "        raise TypeError('contrast_normalize requires flat patches')\n",
      "    if remove_mean:\n",
      "        xm = X.mean(1)\n",
      "    else:\n",
      "        xm = X[:,0] * 0\n",
      "    Xc = X - xm[:, None]\n",
      "    l2 = (Xc * Xc).sum(axis=1)\n",
      "    figure()\n",
      "    title('l2 of image patches')\n",
      "    hist(l2)\n",
      "    if hard_beta:\n",
      "        div2 = np.maximum(l2, beta)\n",
      "    else:\n",
      "        div2 = l2 + beta\n",
      "    X = Xc / np.sqrt(div2[:, None])\n",
      "    return X\n",
      "\n",
      "def ZCA_whiten(patches):\n",
      "    # -- ZCA whitening (with band-pass)\n",
      "\n",
      "    # Algorithm from Coates' sc_vq_demo.m\n",
      "\n",
      "    X = patches.reshape(len(patches), -1).astype('float64')\n",
      "\n",
      "    X = contrast_normalize(X)\n",
      "    print 'patch_whitening_filterbank_X starting ZCA'\n",
      "    M, _std = mean_and_std(X)\n",
      "    Xm = X - M\n",
      "    assert Xm.shape == X.shape\n",
      "    print 'patch_whitening_filterbank_X starting ZCA: dot', Xm.shape\n",
      "    C = np.dot(Xm.T, Xm) / (Xm.shape[0] - 1)\n",
      "    print 'patch_whitening_filterbank_X starting ZCA: eigh'\n",
      "    D, V = np.linalg.eigh(C)\n",
      "    figure()\n",
      "    plot(D)\n",
      "    title('Eigenspectrum of image patches')\n",
      "    print 'patch_whitening_filterbank_X starting ZCA: dot', V.shape\n",
      "    P = np.dot(np.sqrt(1.0 / (D + gamma)) * V, V.T)\n",
      "    assert M.ndim == 1\n",
      "    return M, P, X\n",
      "\n",
      "ZCA_M, ZCA_P, ZCA_X = ZCA_whiten(x_patches_flat)\n",
      "x_white_flat = dot(ZCA_X - ZCA_M, ZCA_P)\n",
      "figure()\n",
      "show_flat_rgb(x_white_flat[:100], (10, 10))\n",
      "title('ZCA-whitened image patches')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- TRAIN THE AUTOENCODER ON WHITENED PATCHES\n",
      "\n",
      "# Remember to re-initialize the model parameters if you were training them\n",
      "# on the raw patches earlier\n",
      "\n",
      "whitened_streams={'x': x_white_flat.reshape((n_examples, 1, n_visible))}\n",
      "W, b, c = autodiff.fmin_sgd(training_criterion,\n",
      "        args=(W, b, c),\n",
      "        streams=whitened_streams,\n",
      "        stepsize=0.01,\n",
      "        loops=5,  # -- fmin_sgd can iterate over the streams repeatedly\n",
      "        print_interval=1000,\n",
      "        )\n",
      "show_flat_rgb(W.T, n_hidden2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}