{
 "metadata": {
  "name": "2_3_convnet"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Convolutional Networks\n",
      "\n",
      "Convolutional Neural Networks (ConvNets) are MLP variants which are\n",
      "specialized for image processing.\n",
      "From Hubel and Wiesel's early work on the cat's visual cortex [Hubel68]_,\n",
      "we know there exists a complex arrangement of cells within the visual cortex.\n",
      "These cells are sensitive to small sub-regions of the input space, called a\n",
      "_receptive field_, and are tiled in such a way as to cover the entire visual\n",
      "field. These filters are local in input space and are thus better suited to\n",
      "exploit the strong spatially local correlation present in natural images.\n",
      "\n",
      "Additionally, two basic cell types have been identified: simple cells (S) and\n",
      "complex cells (C). Simple cells (S) respond maximally to specific edge-like\n",
      "stimulus patterns within their receptive field. Complex cells (C) have larger\n",
      "receptive fields and are locally invariant to the exact position of the\n",
      "stimulus."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sparse Connectivity\n",
      "\n",
      "ConvNets (as well as several related computer vision architectures XXX),\n",
      "exploit spatially local correlation by enforcing a local connectivity pattern between\n",
      "neurons of adjacent layers. The input hidden units in the m-th layer are\n",
      "connected to a local subset of units in the (m-1)-th layer, which have spatiallycontiguous receptive fields. We can illustrate this graphically as follows:\n",
      "\n",
      "<img src=\"files/images/sparse_1D_nn.png\", align=center/>\n",
      "\n",
      "Imagine that layer **m-1** is the input retina.\n",
      "In the above, units in layer **m**\n",
      "have receptive fields of width 3 with respect to the input retina and are thus only\n",
      "connected to 3 adjacent neurons in the layer below (the retina).\n",
      "Units in layer **m** have\n",
      "a similar connectivity with the layer below. We say that their receptive\n",
      "field with respect to the layer below is also 3, but their receptive field\n",
      "with respect to the input is larger (it is 5).\n",
      "The architecture thus\n",
      "confines the learnt \"filters\" (corresponding to the input producing the strongest response) to be a spatially local pattern\n",
      "(since each unit is unresponsive to variations outside of its receptive field with respect to the retina).\n",
      "As shown above, stacking many such\n",
      "layers leads to \"filters\" (not anymore linear) which become increasingly \"global\" however (i.e\n",
      "spanning a larger region of pixel space). For example, the unit in hidden\n",
      "layer **m+1** can encode a non-linear feature of width 5 (in terms of pixel\n",
      "space)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Shared Weights\n",
      "\n",
      "In ConvNets, each sparse filter $h_i$ is additionally replicated across the\n",
      "entire visual field. These \"replicated\" units form a **feature map**, which\n",
      "share the same parametrization, i.e. the same weight vector and the same bias.\n",
      "\n",
      "<img src=\"files/images/conv_1D_nn.png\" align=center/>\n",
      "\n",
      "In the above figure, we show 3 hidden units belonging to the same feature map.\n",
      "Weights of the same color are shared, i.e. are constrained to be identical.\n",
      "Gradient descent can still be used to learn such shared parameters, and\n",
      "requires only a small change to the original algorithm. The gradient of a\n",
      "shared weight is simply the sum of the gradients of the parameters being\n",
      "shared.\n",
      "\n",
      "Why are shared weights interesting ? Replicating units in this way allows for\n",
      "features to be detected regardless of their position in the visual field.\n",
      "Additionally, weight sharing offers a very efficient way to do this, since it\n",
      "greatly reduces the number of free parameters to learn. By controlling model\n",
      "capacity, ConvNets tend to achieve better generalization on vision problems.\n",
      "\n",
      "### Details and Notation\n",
      "\n",
      "Conceptually, a feature map is obtained by convolving the input image with a\n",
      "linear filter, adding a bias term and then applying a non-linear function. If\n",
      "we denote the k-th feature map at a given layer as $h^k$, whose filters\n",
      "are determined by the weights $W^k$ and bias $b_k$, then the\n",
      "feature map $h^k$ is obtained as follows (for $tanh$ non-linearities):\n",
      "\n",
      "$$\n",
      "    h^k_{ij} = \\tanh ( (W^k * x)_{ij} + b_k ).\n",
      "$$\n",
      "\n",
      "To form a richer representation of the data, hidden layers are composed of\n",
      "a set of multiple feature maps, $\\{h^{(k)}, k=0..K\\}$.\n",
      "The weights $W$ of this layer can be parametrized as a 4D tensor\n",
      "(destination feature map index, source feature map index, source vertical position index, source horizontal position index)\n",
      "and\n",
      "the biases $b$ as a vector (one element per destination feature map index).\n",
      "We illustrate this graphically as follows:\n",
      "\n",
      "<img src=\"files/images/cnn_explained.png\" align=center />\n",
      "\n",
      "**Figure 1**: example of a convolutional layer\n",
      "\n",
      "Here, we show two layers of a ConvNet, containing 4 feature maps at layer (m-1)\n",
      "and 2 feature maps ($h^0$ and $h^1$) at layer m. Pixels (neuron outputs) in\n",
      "$h^0$ and $h^1$ (outlined as blue and red squares) are computed\n",
      "from pixels of layer (m-1) which fall within their 2x2 receptive field in the\n",
      "layer below (shown\n",
      "as colored rectangles). Notice how the receptive field spans all four input\n",
      "feature maps. The weights $W^0$ and $W^1$ of $h^0$ and\n",
      "$h^1$ are thus 3D weight tensors. The leading dimension indexes the\n",
      "input feature maps, while the other two refer to the pixel coordinates.\n",
      "\n",
      "Putting it all together, $W^{kl}_{ij}$ denotes the weight connecting\n",
      "each pixel of the k-th feature map at layer m, with the pixel at coordinates\n",
      "(i,j) of the l-th feature map of layer (m-1)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import time                                                                    \n",
      "                                                                               \n",
      "import numpy as np\n",
      "from numpy import arange, dot, maximum, ones, tanh, zeros\n",
      "from numpy.random import uniform                                               \n",
      "\n",
      "from skdata import mnist                                                       \n",
      "import autodiff\n",
      "\n",
      "from util import show_filters\n",
      "from util import hinge                                                         \n",
      "from util import ova_svm_prediction, ova_svm_cost                              \n",
      "from util import tanh_layer\n",
      "from util import mlp_prediction, mlp_cost\n",
      "\n",
      "# -- filterbank normalized cross-correlation                                   \n",
      "from util import fbncc \n",
      "\n",
      "# -- max-pooling over 2x2 windows                                              \n",
      "from util import max_pool_2d_2x2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- load and prepare the data set\n",
      "#\n",
      "# -- N.B. we're loading up x as images this time, not vectors\n",
      "dtype = 'float32'  # helps save memory and go faster\n",
      "n_examples = 10000\n",
      "data_view = mnist.views.OfficialImageClassification(x_dtype=dtype)\n",
      "n_classes = 10\n",
      "x_rows, x_cols = data_view.train.x.shape[1:3]\n",
      "# -- N.B. we shuffle the input to have shape\n",
      "#    (#examples, #channels, #rows, #cols)\n",
      "x = data_view.train.x[:n_examples].transpose(0, 3, 1, 2)\n",
      "y = data_view.train.y[:n_examples]\n",
      "y1 = -1 * ones((len(y), n_classes)).astype(dtype)\n",
      "y1[arange(len(y)), y] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- similar to tanh_layer, but we used fbncc instead of dot\n",
      "def tanh_conv_layer(W_fb, b_fb, img4):\n",
      "    activation = fbncc(img4, W_fb) + b_fb\n",
      "    activation = max_pool_2d_2x2(activation)\n",
      "    return np.tanh(activation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- top-level parameters of this script\n",
      "\n",
      "n_filters = 16\n",
      "patch_height = 5\n",
      "patch_width = 5\n",
      "patch_size = patch_height * patch_width\n",
      "\n",
      "# -- allocate one convolutional layer\n",
      "W_fb = uniform(\n",
      "        low=-np.sqrt(6.0 / (patch_size + n_filters)),\n",
      "        high=np.sqrt(6.0 / (patch_size + n_filters)),\n",
      "        size=(n_filters, 1, patch_height, patch_width),\n",
      "        ).astype(x.dtype)\n",
      "b_fb = zeros((\n",
      "            n_filters,\n",
      "            x_rows - patch_height + 1,\n",
      "            x_cols - patch_width + 1),\n",
      "        dtype=x.dtype)\n",
      "\n",
      "# initialize input layer parameters\n",
      "n_hidden = 200\n",
      "\n",
      "V = uniform(low=-np.sqrt(6.0 / (b_fb.size // 4 + n_hidden)),\n",
      "                high=np.sqrt(6.0 / (b_fb.size // 4 + n_hidden)),\n",
      "                size=(b_fb.size // 4, n_hidden)).astype(x.dtype)\n",
      "c = zeros(n_hidden, dtype=x.dtype)\n",
      "\n",
      "# now allocate the SVM at the top\n",
      "W = zeros((n_hidden, n_classes), dtype=x.dtype)\n",
      "b = zeros(n_classes, dtype=x.dtype)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convnet_prediction(W_fb, b_fb, V, c, W, b, x):\n",
      "    layer1 = tanh_conv_layer(W_fb, b_fb, x)\n",
      "    layer1_size = np.prod(layer1.shape[1:])\n",
      "    layer2 = tanh_layer(V, c,\n",
      "            np.reshape(layer1, (x.shape[0], layer1_size)))\n",
      "    prediction = ova_svm_prediction(W, b, layer2)\n",
      "    return prediction\n",
      "\n",
      "def convnet_cost(W_fb, b_fb, V, c, W, b, x, y1):\n",
      "    layer1 = tanh_conv_layer(W_fb, b_fb, x)\n",
      "    layer1_size = np.prod(layer1.shape[1:])\n",
      "    layer2 = tanh_layer(V, c,\n",
      "            np.reshape(layer1, (x.shape[0], layer1_size)))\n",
      "    cost = ova_svm_cost(W, b, layer2, y1)\n",
      "    return cost\n",
      "\n",
      "print convnet_cost(W_fb, b_fb, V, c, W, b, x[:3], y1[:3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Filterbank Normalized Cross-Correlation\n",
      "\n",
      "Let's have a little bit of fun with this...\n",
      "\n",
      ".. code-block:: python\n",
      "\n",
      "        import pylab\n",
      "        from PIL import Image\n",
      "\n",
      "        # open random image of dimensions 639x516\n",
      "        img = Image.open(open('images/3wolfmoon.jpg'))\n",
      "        img = numpy.asarray(img, dtype='float64') / 256.\n",
      "\n",
      "        # put image in 4D tensor of shape (1, 3, height, width)\n",
      "        img_ = img.swapaxes(0, 2).swapaxes(1, 2).reshape(1, 3, 639, 516)\n",
      "        filtered_img = f(img_)\n",
      "\n",
      "        # plot original image and first and second components of output\n",
      "        pylab.subplot(1, 3, 1); pylab.axis('off'); pylab.imshow(img)\n",
      "        pylab.gray();\n",
      "        # recall that the convOp output (filtered image) is actually a \"minibatch\",\n",
      "        # of size 1 here, so we take index 0 in the first dimension:\n",
      "        pylab.subplot(1, 3, 2); pylab.axis('off'); pylab.imshow(filtered_img[0, 0, :, :])\n",
      "        pylab.subplot(1, 3, 3); pylab.axis('off'); pylab.imshow(filtered_img[0, 1, :, :])\n",
      "        pylab.show()\n",
      "\n",
      "Notice that a randomly initialized filter acts very much like an edge detector!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also of note, remark that we use the same weight initialization formula as\n",
      "with the MLP. Weights are sampled randomly from a uniform distribution in the\n",
      "range [-1/$D_0$, 1/$D_0$], where $D_0$ is the number of inputs to a hidden\n",
      "unit. For MLPs, this was the number of units in the layer below. For ConvNets\n",
      "however, we have to take into account the number of input feature maps and the\n",
      "size of the receptive fields."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Max-Pooling\n",
      "\n",
      "ConvNets emulate the behaviour of \"complex cells\" in visual cortex by\n",
      "_max-pooling_, which is a form of\n",
      "nonlinear downsampling. Max-pooling partitions the input image into\n",
      "a set of non-overlapping rectangles and, for each such sub-region, outputs the\n",
      "maximum value.\n",
      "Max-pooling is useful in vision for two reasons: (1) it reduces the\n",
      "computational complexity for upper layers and (2) it provides a form of\n",
      "translation invariance. To understand the invariance argument, imagine\n",
      "cascading a max-pooling layer with a convolutional layer. There are 8\n",
      "directions in which one can translate the input image by a single pixel. If\n",
      "max-pooling is done over a 2x2 region, 3 out of these 8 possible\n",
      "configurations will produce exactly the same output at the convolutional\n",
      "layer. For max-pooling over a 3x3 window, this jumps to 5/8.\n",
      "\n",
      "Since it provides additional robustness to position, max-pooling is thus a\n",
      "\"smart\" way of reducing the dimensionality of intermediate representations.\n",
      "\n",
      "Max-pooling over the standard 2x2 window is implemented by `util.max_pool_2d_2x2`.\n",
      "It actually uses Theano's implementation of max-pooling internally, there is no native `numpy` implementation of max-pooling."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define the maxpooling or explain why it's in util"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Stacking Layers into a ConvNet\n",
      "\n",
      "A ConvNet architecture interleaves convolutional and pooling layers to produce\n",
      "the input to an MLP.\n",
      "\n",
      "<img src=\"files/images/mylenet.png\" align=center />\n",
      "\n",
      "From an implementation point of view, this means lower-layers operate on 4D\n",
      "tensors. These are then flattened to a 2D matrix of rasterized feature maps,\n",
      "to be compatible with our previous MLP implementation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training a ConvNet"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SGD minimization\n",
      "\n",
      "# -- do n_online_loops passes through the data set doing SGD\n",
      "#    This can be faster at the beginning than L-BFGS\n",
      "t0 = time.time()\n",
      "online_batch_size = 1\n",
      "n_batches = n_examples / online_batch_size\n",
      "W_fb, b_fb, V, c, W, b = autodiff.fmin_sgd(convnet_cost, (W_fb, b_fb, V, c, W, b),\n",
      "            streams={\n",
      "                'x': x.reshape((n_batches, online_batch_size,) + x.shape[1:]),\n",
      "                'y1': y1.reshape((n_batches, online_batch_size, y1.shape[1]))},\n",
      "            loops=2,\n",
      "            stepsize=0.01,\n",
      "            print_interval=1000,\n",
      "            )\n",
      "print 'SGD took %.2f seconds' % (time.time() - t0)\n",
      "show_filters(W_fb.reshape(n_filters, 5 * 5).T, (4, 4), (2, 8))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing the ConvNet\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_predictions = convnet_prediction(W_fb, b_fb, V, c, W, b, x)\n",
      "train_errors = y != train_predictions\n",
      "print 'Current train set error rate', np.mean(train_errors)\n",
      "\n",
      "test_predictions = convnet_prediction(W_fb, b_fb, V, c, W, b, data_view.test.x[:].transpose(0, 3, 1, 2))\n",
      "test_errors = data_view.test.y[:] != test_predictions\n",
      "print 'Current test set error rate', np.mean(test_errors)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercises\n",
      "\n",
      "\n",
      "### Exercise: Hyper-parameter optimization\n",
      "\n",
      "The current hyper-parameters of the convnet are not great, and the test set error is around 4%. Try adjusting hyper-parameters such as:\n",
      "\n",
      "* the SGD learning rate\n",
      "* the SGD mini-batch size\n",
      "* the use of L-BFGS (adapt it from the mlp notebook)\n",
      "* the number of hidden units in the tanh layer\n",
      "* the patch size of the input layer\n",
      "* the number of filters in the convolutional layer\n",
      "* the amount of training data used\n",
      "\n",
      "How all these things affect training and test error?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Adding an internal hidden layer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tips and Tricks\n",
      "\n",
      "### Tips and Tricks: Choosing Hyperparameters\n",
      "\n",
      "ConvNets are especially tricky to train, as they add even more hyper-parameters than\n",
      "a standard MLP. While the usual rules of thumb for learning rates andregularization constants still apply, the following should be kept in mind when\n",
      "optimizing ConvNets.\n",
      "\n",
      "### Tips and Tricks: Number of filters\n",
      "\n",
      "When choosing the number of filters per layer, keep in mind that computing the\n",
      "activations of a single convolutional filter is much more expensive than with\n",
      "traditional MLPs!\n",
      "\n",
      "Assume layer $(l-1)$ contains $K^{l-1}$ feature\n",
      "maps and $M \\times N$ pixel positions (i.e.,\n",
      "number of positions times number of feature maps),\n",
      "and there are $K^l$ filters at layer $l$ of shape $m \\times n$.\n",
      "Then computing a feature map (applying an $m \\times n$ filter\n",
      "at all $(M-m) \\times (N-n)$ pixel positions where the\n",
      "filter can be applied) costs $(M-m) \\times (N-n) \\times m \\times n \\times K^{l-1}$.\n",
      "The total cost is $K^l$ times that. Things may be more complicated if\n",
      "not all features at one level are connected to all features at the previous one.\n",
      "\n",
      "For a standard MLP, the cost would only be $K^l \\times K^{l-1}$\n",
      "where there are $K^l$ different neurons at level $l$.\n",
      "As such, the number of filters used in ConvNets is typically much\n",
      "smaller than the number of hidden units in MLPs and depends on the size of the\n",
      "feature maps (itself a function of input image size and filter shapes).\n",
      "\n",
      "Since feature map size decreases with depth, layers near the input layer will tend to\n",
      "have fewer filters while layers higher up can have much more. In fact, to\n",
      "equalize computation at each layer, the product of the number of features\n",
      "and the number of pixel positions is typically picked to be roughly constant\n",
      "across layers. To preserve the information about the input would require\n",
      "keeping the total number of activations (number of feature maps times\n",
      "number of pixel positions) to be non-decreasing from one layer to the next\n",
      "(of course we could hope to get away with less when we are doing supervised\n",
      "learning). The number of feature maps directly controls capacity and so\n",
      "that depends on the number of available examples and the complexity of \n",
      "the task.\n",
      "\n",
      "\n",
      "### Tips and Tricks: Filter Shape\n",
      "\n",
      "Common filter shapes found in the litterature vary greatly, usually based on\n",
      "the dataset. Best results on MNIST-sized images (28x28) are usually in the 5x5range on the first layer, while natural image datasets (often with hundreds of pixels in each\n",
      "dimension) tend to use larger first-layer filters of shape 12x12 or 15x15.\n",
      "\n",
      "The trick is thus to find the right level of \"granularity\" (i.e. filter\n",
      "shapes) in order to create abstractions at the proper scale, given a\n",
      "particular dataset.\n",
      "\n",
      "\n",
      "### Tips and Tricks: Max Pooling Shape\n",
      "\n",
      "Typical values are 2x2 or no max-pooling. Very large input images may warrant\n",
      "4x4 pooling in the lower-layers. Keep in mind however, that this will reduce the\n",
      "dimension of the signal by a factor of 16, and may result in throwing away too\n",
      "much information."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}