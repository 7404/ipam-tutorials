{
 "metadata": {
  "name": "2_1_linear_svm"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear Support Vector Machine\n",
      "=============================\n",
      "\n",
      "The linear support vector machine (SVM) is a linear classifier parametrized by a matrix of weights $W$ and a bias vector $\\mathbf{b}$.  We will develop the multiclass \"One vs. All\" linear support vector machine, which is a concatentation of two-class support vector machines. We will suppose that our label set is the non-negative integers up to some maximum L."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize the workspace by importing several symbols\n",
      "import logging\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "from numpy import arange, dot, maximum, ones, tanh, zeros\n",
      "from numpy.random import randn\n",
      "\n",
      "from skdata import mnist\n",
      "import autodiff\n",
      "\n",
      "from util import show_filters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- load and prepare the data set (even download if necessary)\n",
      "dtype = 'float32'\n",
      "n_examples = 10000\n",
      "n_classes = 10         # -- denoted L in the math expressions\n",
      "img_shape = (28, 28)\n",
      "\n",
      "data_view = mnist.views.OfficialVectorClassification(x_dtype=dtype)\n",
      "x = data_view.train.x[:n_examples]\n",
      "y = data_view.train.y[:n_examples]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supposing that our input is a vector (generally a _feature vector_) $\\mathbf{x}$, the prediction $l^*$ of the OVA-SVM is the argmax of the affine function:\n",
      "\n",
      "$$\n",
      "l^* = \\operatorname*{argmax}_{l=0}^{L} (\\mathbf{x} W_l + \\mathbf{b}_l)\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ova_svm_prediction(W, b, x):\n",
      "    return np.argmax(np.dot(x, W) + b, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training an SVM\n",
      "\n",
      "The most standard way of training an SVM is to maximize the _margin_ on training data.\n",
      "The _margin_ is always defined in the two-class case (where $y \\in \\\\{-1, +1\\\\}$) \n",
      "as $y * (xW + b)$.\n",
      "The affine function parametrized by $W$ and $b$ defines a hyper-plane where $\\mathbf{x}W + b = 0$ that the SVM interprets as a decision surface.\n",
      "The _margin_ represents how far away $\\mathbf{x}W+b$ is from being on the wrong side of the decision surface: large positive values mean there is a safe distance (\"margin of error?\"), negative values mean that the decision surface is actually incorrect for the given $\\mathbf{x}$.\n",
      "\n",
      "The most standard sense in which SVMs maximize margin is via the _hinge loss_. The hinge loss of margin value $u$ is defined as\n",
      "\n",
      "$$\n",
      "\\mathrm{hinge}(u) = \\max(0, 1-u)\n",
      "$$\n",
      "\n",
      "By maximizing the average hinge loss of the margin of training examples, we maximize a tight convex upper bound on the mis-classification rate (zero-one loss), and can \n",
      "get a good binary classifier using fast algorithms for convex optimization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hinge(u):\n",
      "    return np.maximum(0, 1 - u)\n",
      "\n",
      "ugrid = np.arange(-5, 5, .1)\n",
      "plot(ugrid, hinge(ugrid), label='hinge loss')\n",
      "plot(ugrid, ugrid < 0, label='zero-one loss')\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Multiclass SVM\n",
      "\n",
      "In the multiclass case, it is not clear what the \"correct\" margin definition should be, and several useful ones have been proposed.\n",
      "For the rest of this tutorial we'll develop the \"one vs. all\" multiclass SVM, sometimes called an OVA-SVM.\n",
      "\n",
      "In the OVA-SVM, the training objective is defined by $L$ independent SVMs.\n",
      "We will train an OVA-SVM by converting the integer-valued labels\n",
      "$y$ to $Y \\in \\\\{-1, +1\\\\}^{N \\times L}$ and training $L$ SVMs at once.\n",
      "The $L$ columns of $Y$ represent binary classification tasks.\n",
      "The $L$ columns of $W$ and $L$ elements of $\\mathbf{b}$ store the parameters of the $L$ SVMs.\n",
      "\n",
      "The [unregularized] training objective is:\n",
      "\n",
      "$$\n",
      "\\mathcal{L}(\\mathcal{D}; W, \\mathbf{b}) =\n",
      "\\frac{1}{N}\n",
      "\\sum_{(\\mathbf{x}^{(i)}, Y^{(i)}) \\in \\mathcal{D}}\n",
      "~\n",
      "\\sum_{l=1}^{L}\n",
      "~\n",
      "\\max \\left( 0, 1 - Y^{(i)}_l (\\mathbf{x}^{(i)} W_l + \\mathbf{b}_l) \\right)\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- prepare a \"1-hot\" version of the labels, denoted Y in the math\n",
      "y1 = -1 * ones((len(y), n_classes)).astype(dtype)\n",
      "y1[arange(len(y)), y] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ova_svm_cost(W, b, x, y1):\n",
      "    # -- one vs. all linear SVM loss\n",
      "    margin = y1 * (dot(x, W) + b)\n",
      "    cost = hinge(margin).mean(axis=0).sum()\n",
      "    return cost\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The training itself consists in minimizing the objective $\\mathcal{L}(\\mathcal{D}; W, b)$ with respect to $W$ and $b$.  The criterion is convex, so it doesn't much matter where we start. Zero works well in practice."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize the model\n",
      "W = zeros((x.shape[1], n_classes), dtype=dtype)\n",
      "b = zeros(n_classes, dtype=dtype)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many specialized SVM solvers available, but they tend to be most helpful for non-linear SVMs.  In the linear case a simple combination of stochastic gradient descent (SGD) and BFGS can be very effective."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- do n_online_loops passes through the data set doing SGD\n",
      "#    This is a good pre-conditioning process prior to L-BFGS\n",
      "t0 = time.time()\n",
      "online_batch_size = 1\n",
      "n_online_epochs = 1\n",
      "n_batches = n_examples / online_batch_size\n",
      "W, b = autodiff.fmin_sgd(ova_svm_cost, (W, b),\n",
      "            streams={\n",
      "                'x': x.reshape((n_batches, online_batch_size, x.shape[1])),\n",
      "                'y1': y1.reshape((n_batches, online_batch_size, y1.shape[1]))},\n",
      "            loops=n_online_epochs,\n",
      "            stepsize=0.001,\n",
      "            print_interval=1000,\n",
      "            )\n",
      "print 'SGD took %.2f seconds' % (time.time() - t0)\n",
      "show_filters(W.T, img_shape, (2, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- L-BFGS optimization of our SVM cost.\n",
      "def batch_criterion(W, b):\n",
      "    return ova_svm_cost(W, b, x, y1)\n",
      "W, b = autodiff.fmin_l_bfgs_b(batch_criterion, (W, b), maxfun=20, m=20, iprint=1)\n",
      "#   -- the output from this command comes from Fortran, so iPython does not see it.\n",
      "#      To monitor progress, look at the terminal from which you launched ipython\n",
      "show_filters(W.T, img_shape, (2, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing the SVM\n",
      "\n",
      "Once the a classifier has been trained, we can test it for generalization accuracy on the test set. We test it by making predictions for the examples in the test set and counting up the number of classification mistakes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_predictions = ova_svm_prediction(W, b, x)\n",
      "train_errors = y != train_predictions\n",
      "print 'Current train set error rate', np.mean(train_errors)\n",
      "\n",
      "test_predictions = ova_svm_prediction(W, b, data_view.test.x[:])\n",
      "test_errors = data_view.test.y[:] != test_predictions\n",
      "print 'Current test set error rate', np.mean(test_errors)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SVM Summary\n",
      "\n",
      "The linear Support Vector Machine (SVM) is an accurate and quick-to-train linear classifier.\n",
      "We looked a multiclass variant called a \"One vs. All\" SVM,\n",
      "parameterized by a matrix $W$ of weights and a vector $\\mathbf{b}$ of biases.\n",
      "It is quick to train a linear SVM by gradient descent, especially\n",
      "by pre-conditioning a Quasi-Newton solver (e.g. L-BFGS) with some initial iterations of SGD."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: L1 and L2 regularization\n",
      "\n",
      "Typically linear SVMs are L2-regularized. That means that in addition to $\\mathcal{L}$, we minimize the sum of the squared elements of $W$:\n",
      "\n",
      "$$\n",
      "\\mathcal{L}_{\\ell_2}(\\mathcal{D}; W, b) = \\mathcal{L}(\\mathcal{D}; W, b) + \\alpha ||W||_2^2\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ova_svm_cost_l2(W, b, x):\n",
      "    raise NotImplementedError('implement me')\n",
      "\n",
      "# re-run the SGD and LBFGS training fragments after filling\n",
      "# in this function body to implement an L2-regularized SVM.\n",
      "\n",
      "# In cases where the training data are linearly separable, this can be very important.\n",
      "# How big does \\alpha have to be to make any difference?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Different Data Sets\n",
      "\n",
      "We have been using MNIST so far. How good is a linear classifier on other datasets that are typically used in deep learning?\n",
      "The following two code fragments bring in access to the CIFAR-10 and SVHN data sets.\n",
      "You'll have to access the `shape` attribute of the training images to figure out how many visible units there are, and then re-allocate `W` before repeating the training and testing processes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skdata import cifar10\n",
      "data_view = cifar10.view.OfficialVectorClassification(x_dtype=dtype)\n",
      "\n",
      "# Write the rest of the answer here:\n",
      "# - define x and y for training\n",
      "\n",
      "# Repeat the training steps in the code fragments above to train a linear SVM for CIFAR10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skdata import svhn\n",
      "data_view = svhn.view.OfficialVectorClassification(x_dtype=dtype)\n",
      "\n",
      "# Write the rest of the answer here:\n",
      "# - define x and y for training\n",
      "\n",
      "# Repeat the training steps in the code fragments above to train a linear SVM for\n",
      "# Street View House Numbers (SVHN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bonus: Pixel colors in cifar10 and svhn are encoded as RGB triples.  There are many other ways of representing color. Some of these are linearly related to RGB, others non-linearly.\n",
      "The [scikit-image](http://scikits-image.org/) project defines a number of color-conversion routines.\n",
      "Are any of these color representations better than RGB for image classification?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skimage.color import convert_colorspace\n",
      "\n",
      "# Write your answer here\n",
      "# use convert_colorspace to define new versions of the training and testing images.\n",
      "\n",
      "# Hint - you'll have to un-rasterize the x matrix into a N-dimensional array\n",
      "#        n. examples x n. rows x n. cols x n. channels\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Variations on the Hinge Loss\n",
      "\n",
      "You don't have to use the hinge loss to train a linear classifier. Other popular loss functions include the squared hinge loss, the \"Huberized\" hinge loss, and the exponential function.  These functions are continuously differentiable, which you might expect to help for gradient-based optimization. Does it help? How else are these functions different? In what cases would it matter?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Ranking SVM\n",
      "\n",
      "There is another popular version of multiclass SVMs based on a different loss function. In this alternative, the margin is defined as the difference in $(\\mathbf{x}W + \\mathbf{b})$ between just two classes rather than all the classes. The two classes are (i) the correct class, and (j) the highest-ranked class other than the correct class.\n",
      "\n",
      "If $\\mathbf{h} = \\mathbf{x}W + \\mathbf{b}$, the margin is then defined as $1 - \\mathbf{h}_i + \\mathbf{h}_j$.\n",
      "It means that we want the correct class to be at least 1 unit of \"margin\" ahead of the next-best alternative.\n",
      "\n",
      "This loss is especially interesting for setting in which there are many potential labels, especially if you have a quick way of identifying the (j) label.\n",
      "\n",
      "Can you code this up as a training criterion and use it?\n",
      "\n",
      "HINT: Start with the case of pure stochastic gradient descent, I think the batch case requires numpy's _advanced indexing_ which not currently supported by `autodiff`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}