{
 "metadata": {
  "name": "3_1_unsupervised_features"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# imports -- run this once after every kernel restart\n",
      "from functools import partial\n",
      "import logging\n",
      "import sys\n",
      "import time \n",
      "\n",
      "import numpy as np\n",
      "from numpy import dot, exp, log, newaxis, sqrt, tanh \n",
      "from numpy.random import rand\n",
      "\n",
      "from skdata import mnist, cifar10, streetsigns\n",
      "import autodiff\n",
      "\n",
      "from utils import show_filters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions -- run this once after a kernel restart\n",
      "# Re-run it after any change you make to these routines.\n",
      "\n",
      "def euclidean_distances2(X, Y):\n",
      "    \"\"\"Return all-pairs squared distances between rows of X and Y\n",
      "    \"\"\"\n",
      "    # N.B. sklearn.metrics.pairwise.euclidean_distances\n",
      "    # offers a more robust version of this routine,\n",
      "    # but which does things that autodiff currently does not support.\n",
      "    XX = np.sum(X * X, axis=1)[:, newaxis]\n",
      "    YY = np.sum(Y * Y, axis=1)[newaxis, :]\n",
      "    distances = XX - 2 * dot(X, Y.T) + YY\n",
      "    np.maximum(distances, 0, distances)\n",
      "    return distances\n",
      "    \n",
      "\n",
      "def cross_entropy(x, x_rec_p):\n",
      "    \"\"\"Return the independent Bernoulli cross-entropy cost\n",
      "\n",
      "    x_rec_p is the Bernoulli parameter of the model's reconstruction\n",
      "    \"\"\"\n",
      "    # -- N.B. this is numerically bad, we're counting on Theano to fix up\n",
      "    return -(x * log(x_rec_p) + (1 - x) * log(1 - x_rec_p)).sum(axis=1)\n",
      "\n",
      "\n",
      "def logistic(x):\n",
      "    \"\"\"Return logistic sigmoid of float or ndarray `x`\"\"\"\n",
      "    return 1.0 / (1.0 + exp(-x))\n",
      "\n",
      "\n",
      "def softmax(x):\n",
      "    \"\"\"Return the softmax of each row in x\"\"\"\n",
      "    x2 = x - x.max(axis=1)[:, newaxis]\n",
      "    ex = exp(x2)\n",
      "    return ex / ex.sum(axis=1)[:, newaxis]\n",
      "\n",
      "\n",
      "def squared_error(x, x_rec):\n",
      "    \"\"\"Return the squared error of approximating `x` with `x_rec`\"\"\"\n",
      "    return ((x - x_rec) ** 2).sum(axis=1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pca_autoencoder_real_x(x, w, hidbias, visbias):\n",
      "    hid = dot(x - visbias, w)\n",
      "    x_rec = dot(hid, w.T)\n",
      "    cost = squared_error(x - visbias, x_rec)\n",
      "    return cost, hid\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logistic_autoencoder_binary_x(x, w, hidbias, visbias):\n",
      "    hid = logistic(dot(x, w) + hidbias)\n",
      "    # -- using w.T here is called using \"tied weights\"\n",
      "    # -- using a second weight matrix here is called \"untied weights\"\n",
      "    x_rec = logistic(dot(hid, w.T) + visbias)\n",
      "    cost = cross_entropy(x, x_rec)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def denoising_autoencoder_binary_x(x, w, hidbias, visbias, noise_level):\n",
      "    # -- corrupt the input by zero-ing out some values randomly\n",
      "    noisy_x = x * (rand(*x.shape) > noise_level)\n",
      "    hid = logistic(dot(noisy_x, w) + hidbias)\n",
      "    # -- using w.T here is called using \"tied weights\"\n",
      "    # -- using a second weight matrix here is called \"untied weights\"\n",
      "    x_rec = logistic(dot(hid, w.T) + visbias)\n",
      "    cost = cross_entropy(x, x_rec)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rbm_binary_x(x, w, hidbias, visbias):\n",
      "    hid = logistic(dot(x, w) + hidbias)\n",
      "    hid_sample = (hid > rand(*hid.shape)).astype(x.dtype)\n",
      "\n",
      "    # -- N.B. model is not actually trained to reconstruct x\n",
      "    x_rec = logistic(dot(hid_sample, w.T) + visbias)\n",
      "    x_rec_sample = (x_rec > rand(*x_rec.shape)).astype(x.dtype)\n",
      "\n",
      "    # \"negative phase\" hidden unit expectation\n",
      "    hid_rec = logistic(dot(x_rec_sample, w) + hidbias)\n",
      "\n",
      "    def free_energy(xx):\n",
      "        xw_b = dot(xx, w) + hidbias\n",
      "        return -log(1 + exp(xw_b)).sum(axis=1) - dot(xx, visbias)\n",
      "\n",
      "    cost = free_energy(x) - free_energy(x_rec_sample)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def k_means_real_x(x, w, hidbias, visbias):\n",
      "    xw = euclidean_distances2(x - visbias, w.T)\n",
      "    # -- This calculates a hard winner\n",
      "    hid = (xw == xw.min(axis=1)[:, newaxis])\n",
      "    x_rec = dot(hid, w.T)\n",
      "    cost = ((x - x_rec) ** 2).mean(axis=1)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FISTA = NotImplementedError\n",
      "# real-real Sparse Coding\n",
      "def sparse_coding_real_x(x, w, hidbias, visbias, sparse_coding_algo=FISTA):\n",
      "    # -- several sparse coding algorithms have been proposed, but they all\n",
      "    # give rise to a feature learning algorithm that looks like this:\n",
      "    hid = sparse_coding_algo(x, w)\n",
      "    x_rec = dot(hid, w.T) + visbias\n",
      "    cost = ((x - x_rec) ** 2).mean(axis=1)\n",
      "    # -- the gradient on this cost wrt `w` through the sparse_coding_algo is\n",
      "    # often ignored. At least one notable exception is the work of Karol\n",
      "    # Greggor.  I feel like the Implicit Differentiation work of Drew Bagnell\n",
      "    # is another, but I'm not sure.\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- top-level parameters of this script\n",
      "n_hidden1 = n_hidden2 = 25\n",
      "dtype = 'float32'\n",
      "n_examples = 10000\n",
      "online_batch_size = 1\n",
      "online_epochs = 3\n",
      "\n",
      "# -- TIP: partial creates a new function with some parameters filled in\n",
      "# algo = partial(denoising_autoencoder_binary_x, noise_level=0.3)\n",
      "algo = logistic_autoencoder_binary_x\n",
      "\n",
      "batch_epochs = 10\n",
      "lbfgs_m = 20\n",
      "\n",
      "n_hidden = n_hidden1 * n_hidden2\n",
      "rng = np.random.RandomState(123)\n",
      "\n",
      "data_view = mnist.views.OfficialVectorClassification(x_dtype=dtype)\n",
      "x = data_view.train.x[:n_examples]\n",
      "n_examples, n_visible = x.shape\n",
      "x_img_res = 28, 28\n",
      "\n",
      "# -- uncomment this line to see sample images from the data set\n",
      "# show_filters(x[:100], x_img_res, (10, 10))\n",
      "\n",
      "# -- create a new model  (w, visbias, hidbias)\n",
      "w = rng.uniform(\n",
      "        low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        size=(n_visible, n_hidden)).astype(dtype)\n",
      "visbias = np.zeros(n_visible).astype(dtype)\n",
      "hidbias = np.zeros(n_hidden).astype(dtype)\n",
      "\n",
      "# show_filters(w.T, x_img_res, (n_hidden1, n_hidden2))\n",
      "x_stream = x.reshape((\n",
      "    n_examples / online_batch_size,\n",
      "    online_batch_size,\n",
      "    x.shape[1]))\n",
      "\n",
      "def train_criterion(ww, hbias, vbias, x_i=x):\n",
      "    cost, hid = algo(x_i, ww, hbias, vbias)\n",
      "    l1_cost = abs(ww).sum() * 0.0    # -- raise 0.0 to enforce l1 penalty\n",
      "    l2_cost = (ww ** 2).sum() * 0.0  # -- raise 0.0 to enforce l2 penalty\n",
      "    return cost.mean() + l1_cost + l2_cost\n",
      "\n",
      "# -- ONLINE TRAINING\n",
      "for epoch in range(online_epochs):\n",
      "    t0 = time.time()\n",
      "    w, hidbias, visbias = autodiff.fmin_sgd(train_criterion,\n",
      "            args=(w, hidbias, visbias),\n",
      "            stream=x_stream,  # -- fmin_sgd will loop through this once\n",
      "            stepsize=0.005,   # -- QQ: you should always tune this\n",
      "            print_interval=1000,\n",
      "            )\n",
      "    print 'Online training epoch %i took %f seconds' % (\n",
      "            epoch, time.time() - t0)\n",
      "    show_filters(w.T, x_img_res, (n_hidden1, n_hidden2))\n",
      "\n",
      "# -- BATCH TRAINING\n",
      "w, hidbias, visbias = autodiff.fmin_l_bfgs_b(train_criterion,\n",
      "        args=(w, hidbias, visbias),\n",
      "        # -- scipy.fmin_l_bfgs_b kwargs follow\n",
      "        maxfun=batch_epochs,\n",
      "        iprint=1,     # -- 1 for verbose, 0 for normal, -1 for quiet\n",
      "        m=lbfgs_m,         # -- how well to approximate the Hessian\n",
      "        )\n",
      "\n",
      "show_filters(w.T, x_img_res, (n_hidden1, n_hidden2)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}