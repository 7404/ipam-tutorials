{
 "metadata": {
  "name": "2_2_mlp"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multilayer Perceptron\n",
      "=====================\n",
      "\n",
      "In the previous tutorial on linear SVMs, we looked at how training a model\n",
      "consists in defining a training objective, and then minimizing it with respect\n",
      "to model parameters by either stochastic or batch gradient descent.\n",
      "\n",
      "In this tutorial we will augment our SVM with _internal layers_ of _hidden\n",
      "units_ and turn our linear classifier into a multi-layer architecture.\n",
      "An MLP with one internal layer is sometimes called a _shallow_ architecture,\n",
      "in contrast to an MLP with more internal layers which is sometimes called a _deep_ architecture.\n",
      "\n",
      "An MLP can be viewed as an SVM, where the input $\\mathbf{x}$ is first transformed using a\n",
      "learnt non-linear vector-valued transformation $\\Phi$.\n",
      "The purpose of this transformation is to map the input data into a space where it becomes linearly separable.\n",
      "The vector $\\Phi(\\mathbf{x})$ is referred to as a _hidden layer_.\n",
      "\n",
      "This tutorial will again tackle the problem of MNIST digit classification.\n",
      "XXX"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "from numpy import arange, dot, maximum, ones, tanh, zeros\n",
      "from numpy.random import uniform\n",
      "\n",
      "from skdata import mnist\n",
      "import autodiff\n",
      "\n",
      "from util import show_filters\n",
      "\n",
      "from util import hinge\n",
      "from util import ova_svm_prediction\n",
      "from util import ova_svm_cost"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- load and prepare the data set (even download if necessary)\n",
      "dtype = 'float32'\n",
      "n_examples = 10000\n",
      "n_classes = 10         # -- denoted L in the math expressions\n",
      "img_shape = (28, 28)\n",
      "\n",
      "data_view = mnist.views.OfficialVectorClassification(x_dtype=dtype)\n",
      "x = data_view.train.x[:n_examples]\n",
      "y = data_view.train.y[:n_examples]\n",
      "# -- prepare a \"1-hot\" version of the labels, denoted Y in the math\n",
      "y1 = -1 * ones((len(y), n_classes)).astype(dtype)\n",
      "y1[arange(len(y)), y] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The MLP Model\n",
      "\n",
      "Typically the function $\\Phi$ is taken to be the function\n",
      "$\\mathbb{R}^{D_0} \\rightarrow (-1, 1)^{D_1}$\n",
      "\n",
      "$$\n",
      "\\Phi(\\mathbf{x}; V, \\mathbf{c}) = \\tanh( \\mathbf{x} V + \\mathbf{c})\n",
      "$$\n",
      "\n",
      "in which $V \\in \\mathbb{R}^{D_0 \\times D_1}$ is called a _weight matrix_, and\n",
      "$\\mathbf{c} \\in \\mathbb{R}^{D_1}$ is called a _bias vector_.\n",
      "The integer $D_0$ is the number of elements in $\\mathbf{x}$ (sometimes, number\n",
      "of \"input units\").\n",
      "The integer $D_1$ is the number of \"hidden units\" of the MLP.\n",
      "We abuse notation slightly here by using $\\tanh(\\mathbf{u})$ for a vector\n",
      "$\\mathbf{u}$ to denote\n",
      "the vector of values $\\tanh(\\mathbf{u}_i)$.\n",
      "Sometimes other non-linear scalar functions are used instead of the tanh\n",
      "function -- whichever one is used is called the _activation function_ of\n",
      "layer $\\Phi$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tanh_layer(V, c, x):\n",
      "    return np.tanh(np.dot(x, V) + c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When combined with an SVM classifier (recall previous tutorial), the full classification model can be written\n",
      "\n",
      "$$\n",
      "    \\mathrm{MLP}(\\mathbf{x}) = \\mathrm{SVM}\\left(\\Phi(\\mathbf{x}; V, \\mathbf{c})); W, \\mathbf{b} \\right)\n",
      "$$\n",
      "\n",
      "This sort of MLP (or Artificial Neural Network - ANN) with a single hidden layer\n",
      "is sometimes represented graphically as follows:\n",
      "\n",
      "<img src=\"files/images/mlp.png\" align=center/>\n",
      "\n",
      "A single hidden layer of this form is sufficient to make the MLP a universal approximator.\n",
      "However we will see shortly\n",
      "that there are benefits to using many such hidden layers (deep learning).\n",
      "See these course notes for an\n",
      "[introduction to MLPs, the back-propagation algorithm, and how to train MLPs](http://www.iro.umontreal.ca/~pift6266/H10/notes/mlp.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define prediction function\n",
      "def mlp_prediction(V, c, W, b, x):\n",
      "    h = tanh_layer(V, c, x)\n",
      "    return ova_svm_prediction(W, b, h)\n",
      "\n",
      "def mlp_cost(V, c, W, b, x, y1):\n",
      "    h = tanh_layer(V, c, x)\n",
      "    return ova_svm_cost(W, b, h, y1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training an MLP\n",
      "\n",
      "To train an MLP, we learn **all** parameters of the model ($W, \\mathbf{b}, V,\n",
      "\\mathbf{c}$) by gradient descent,\n",
      "just as we learned the parameters $W, \\mathbf{b}$ previously when training the\n",
      "SVM.\n",
      "\n",
      "\n",
      "The initial values for the weights of a hidden layer ($V$) should be uniformly\n",
      "sampled from a symmetric interval that depends on the activation function.\n",
      "For the tanh activation function results obtained in [Xavier10]_ show that the\n",
      "interval should be\n",
      "$$\n",
      "\\left[ -\\sqrt{\\frac{6}{D_0 + D_1}}, \\sqrt{\\frac{6}{D_0 + D_1}} \\right]\n",
      "$$\n",
      "\n",
      "For the logistic sigmoid function $1 / (1 + e^{-u})$ the interval is slightly\n",
      "different:\n",
      "\n",
      "$$\n",
      "\\left[ -4\\sqrt{\\frac{6}{D_0 + D_1}},4\\sqrt{\\frac{6}{D_0 + D_1}} \\right]\n",
      "$$.\n",
      "\n",
      "This initialization ensures that at least early in training, each neuron operates in a regime of its activation function where information can easily be propagated both upward (activations flowing from inputs to outputs) and backward (gradients flowing from outputs to inputs)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize input layer parameters\n",
      "n_inputs = 784 # -- aka D_0\n",
      "n_hidden = 100 # -- aka D_1\n",
      "\n",
      "V = uniform(low=-np.sqrt(6.0 / (n_inputs + n_hidden)),\n",
      "                high=np.sqrt(6.0 / (n_inputs + n_hidden)),\n",
      "                size=(n_inputs, n_hidden)).astype(x.dtype)\n",
      "c = zeros(n_hidden, dtype=x.dtype)\n",
      "\n",
      "# now allocate the SVM at the top\n",
      "W = zeros((n_hidden, n_classes), dtype=x.dtype)                           \n",
      "b = zeros(n_classes, dtype=x.dtype)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As before, we train this model using stochastic gradient descent with\n",
      "mini-batches. The difference is that we modify the cost function to include the\n",
      "regularization term. `L1_reg` and `L2_reg` are the hyperparameters\n",
      "controlling the weight of these regularization terms in the total cost function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SGD minimization\n",
      "\n",
      "# -- do n_online_loops passes through the data set doing SGD\n",
      "#    This can be faster at the beginning than L-BFGS\n",
      "t0 = time.time()\n",
      "online_batch_size = 1\n",
      "n_batches = n_examples / online_batch_size\n",
      "V, c, W, b = autodiff.fmin_sgd(mlp_cost, (V, c, W, b),\n",
      "            streams={\n",
      "                'x': x.reshape((n_batches, online_batch_size, x.shape[1])),\n",
      "                'y1': y1.reshape((n_batches, online_batch_size, y1.shape[1]))},\n",
      "            loops=5,\n",
      "            stepsize=0.01,\n",
      "            print_interval=1000,\n",
      "            )\n",
      "print 'SGD took %.2f seconds' % (time.time() - t0)\n",
      "show_filters(V.T, img_shape, (10, 10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# L-BFGS minimization\n",
      "\n",
      "def batch_criterion(V, c, W, b):\n",
      "    return mlp_cost(V, c, W, b, x, y1)\n",
      "\n",
      "V, c, W, b = autodiff.fmin_l_bfgs_b(batch_criterion, (V, c, W, b), maxfun=20, m=20, iprint=1)\n",
      "\n",
      "print 'final_cost', batch_criterion(V, c, W, b)\n",
      "# -- N. B. the output from this command comes from Fortran, so iPython does not see it.\n",
      "#    To monitor progress, look at the terminal from which you launched ipython\n",
      "show_filters(V.T, img_shape, (10, 10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing the MLP\n",
      "\n",
      "Once the a classifier has been trained, we can test it for generalization accuracy on the test set. We test it by making predictions for the examples in the test set and counting up the number of classification mistakes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_predictions = mlp_prediction(V, c, W, b, x)\n",
      "train_errors = y != train_predictions\n",
      "print 'Current train set error rate', np.mean(train_errors)\n",
      "\n",
      "test_predictions = mlp_prediction(V, c, W, b, data_view.test.x[:])\n",
      "test_errors = data_view.test.y[:] != test_predictions\n",
      "print 'Current test set error rate', np.mean(test_errors)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercises\n",
      "\n",
      "Try the following exercises out to get a better feel for training MLPs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Breaking symmetry\n",
      "\n",
      "It might seem more natural to initialize all of the MLP parameters to $0$.\n",
      "What goes wrong when you do this?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: SGD vs. L-BFGS\n",
      "\n",
      "Try running L-BFGS starting from the original random initialization -- how good does it get on train and test, and how long does it take to find a solution.\n",
      "\n",
      "Now repeat that with various amounts of SGD before the L-BFGS. What happens?\n",
      "\n",
      "Now raise the number of examples seen per iteration of SGD. How does this affect the convergence of SGD and its value as a pre-conditioner?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Overfitting\n",
      "\n",
      "Regularization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tips and Tricks for training MLPs\n",
      "\n",
      "There are several hyper-parameters in the above code, which are not (and,\n",
      "generally speaking, cannot be) optimized by gradient descent.\n",
      "The design of outer-loop algorithms for optimizing them is a topic of ongoing\n",
      "research.\n",
      "Over the last 25 years, researchers have devised various rules of thumb for choosing them.\n",
      "A very good overview of these tricks can be found in [Efficient\n",
      "BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) by Yann LeCun,\n",
      "Leon Bottou, Genevieve Orr, and Klaus-Robert Mueller. Here, we summarize\n",
      "the same issues, with an emphasis on the parameters and techniques that we\n",
      "actually used in our code.\n",
      "\n",
      "### Tips and Tricks: Nonlinearity\n",
      "\n",
      "Which non-linear activation function should you use in a neural network?\n",
      "Two of the most common ones are the logistic sigmoid and the tanh functions.\n",
      "For reasons explained in [Section 4.4](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf), nonlinearities that\n",
      "are symmetric around the origin are preferred because they tend to produce\n",
      "zero-mean inputs to the next layer (which is a desirable property).\n",
      "Empirically, we have observed that the tanh has better convergence properties.\n",
      "\n",
      "\n",
      "### Tips and Tricks: Weight initialization\n",
      "\n",
      "At initialization we want the weights to be small enough around the origin\n",
      "so that the activation function operates near its linear regime, where gradients are\n",
      "the largest. Otherwise, the gradient signal used for learning is attenuated by\n",
      "each layer as it is propagated from the classifier towards the inputs.\n",
      "Other desirable properties, especially for deep networks,\n",
      "are to conserve variance of the activation as well as variance of back-propagated gradients from layer to layer.\n",
      "This allows information to flow well upward and downward in the network and\n",
      "reduces discrepancies between layers.\n",
      "The initialization used above represents a good compromise between these two\n",
      "constraints.\n",
      "For mathematical considerations, please refer to [Xavier10]_.\n",
      "\n",
      "\n",
      "### Tips and Tricks: Learning Rate\n",
      "\n",
      "Optimization by stochastic gradient descent is very sensitive to the step size or _learning rate_.\n",
      "There is a great deal of literature on how to choose a the learning rate, and how to change it during optimization.\n",
      "The simplest solution is to use a constant rate. Rule of thumb: try\n",
      "several log-spaced values ($10^{-1}, 10^{-2}, \\ldots$) and narrow the\n",
      "(logarithmic) grid search to the region where you obtain the lowest\n",
      "validation error.\n",
      "\n",
      "Decreasing the learning rate over time can help a model to settle down into a\n",
      "[local] minimum.\n",
      "One simple rule for doing that is $\\frac{\\mu_0}{1 + d\\times t}$ where\n",
      "$\\mu_0$ is the initial rate (chosen, perhaps, using the grid search\n",
      "technique explained above), $d$ is a so-called \"decrease constant\"\n",
      "which controls the rate at which the learning rate decreases (typically, a\n",
      "smaller positive number, $10^{-3}$ and smaller) and $t$ is the epoch/stage.\n",
      "\n",
      "[Section 4.7](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) details\n",
      "procedures for choosing a learning rate for each parameter (weight) in our\n",
      "network and for choosing them adaptively based on the error of the classifier.\n",
      "\n",
      "### Tips and Tricks: Number of hidden units\n",
      "\n",
      "The number of hidden units that gives best results is dataset-dependent.\n",
      "Generally speaking, the more complicated the input distribution is, the more capacity the network\n",
      "will require to model it, and so the larger the number of hidden units thatwill be needed (note that the number of weights in a layer, perhaps a more direct\n",
      "measure of capacity, is $D_0\\times D_1$ (recall $D_0$ is the number of inputs and\n",
      "$D_1$ is the number of hidden units).\n",
      "\n",
      "Unless we employ some regularization scheme (early stopping or L1/L2\n",
      "penalties), a typical number of hidden  units vs. generalization performance graph will be U-shaped.\n",
      "\n",
      "### Tips and Tricks: Norm Regularization\n",
      "\n",
      "Typical values to try for the L1/L2 regularization parameter $\\lambda$ are $10^{-2}, 10^{-3}, \\ldots$.\n",
      "It can be useful to regularize the topmost layers in an MLP (closest\n",
      "to and including the classifier itself) to prevent them from overfitting noisy\n",
      "hidden layer features, and to encourage the features themselves to be more\n",
      "discriminative."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}