{
 "metadata": {
  "name": "3_1_autoencoder"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Auto-Encoders and Denoising Auto-Encoders\n",
      "=========================================\n",
      "\n",
      "The auto-encoder (AE) is a classic unsupervised algorithm for non-linear dimensionality reduction.\n",
      "The de-noising auto-encoder (DAE) is an extension of the auto-encoder introduced as a building block for deep networks in [Vincent08].\n",
      "This tutorial introduces the autoencoder as an unsupervised feature learning algorithm for the MNIST data set, and then develops the denoising autoencoder.\n",
      "See section 4.6 of [Bengio09]_ for an overview of auto-encoders.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Before we get started with the rest of the tutorial, \n",
      "# run this once (and again after every kernel restart)\n",
      "# to bring in all the external symbols we'll need.\n",
      "\n",
      "from functools import partial\n",
      "import logging\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "from numpy import dot, exp, log, newaxis, sqrt, tanh \n",
      "from numpy.random import rand\n",
      "\n",
      "from skdata import mnist, cifar10, streetsigns\n",
      "import autodiff\n",
      "\n",
      "from utils import show_filters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's also import the MNIST data set here, so that the examples in the tutorial have some data to work with. (HINT: Some of the exercises will involve using different data sets -- you can use different data sets with the tutorial code by writing a code block like this one that redefining `x` and `x_img_res`.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtype = 'float32'   # -- this sets the working precision for data and model\n",
      "n_examples = 10000  # -- use up to 50000 examples\n",
      "\n",
      "data_view = mnist.views.OfficialImageClassification(x_dtype=dtype)\n",
      "x_as_images = data_view.train.x[:n_examples]\n",
      "x_img_res = x_as_images.shape[1:3]  # -- handy for showing filters\n",
      "x = x.reshape((n_images, -1))       # -- rasterize images to vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Autoencoders\n",
      "------------\n",
      "\n",
      "Generally speaking, an autoencoder maps vector input $\\mathbf{x}$ to some intermediate representation $\\mathbf{y}$ and then back into the original space, in such a way that the end-point is close to $\\mathbf{x}$. While the goal thus defined is simply to learn an identity function,\n",
      "things get interesting when the mappings are parameterized or constrained in such a way that a general identity function is either impossible to represent or at least difficult to learn from data. When this is the case, the goal of learning is to learn a _special-purpose_ identity function that _typically_ works for vectors $\\mathbf{x}$ that we care about, which come from some empirically interesting distribution.  The $\\mathbf{y}$ vector that comes out of this process contains all the _important_ information about $x$ in a new and potentially useful way.\n",
      "\n",
      "In our tutorial here we will deal with vectors $\\mathbf{x}$ that come from the MNIST data set of hand-written digits.\n",
      "Examples from MNIST are vectors $\\mathbf{x} \\in [0,1]^N$,\n",
      "and we will look at the classic one-layer sigmoidal autoencoder parameterized by:\n",
      "\n",
      "* matrix $W \\in \\mathbb{R}^{N \\times M}$ - the _weights_\n",
      "* vector $\\mathbf{b} \\in \\mathbb{R}^M$ - the _bias_\n",
      "* matrix $V \\in \\mathbb{R}^{M \\times N}$ - the _reconstruction weights_\n",
      "* vector $\\mathbf{c} \\in \\mathbb{R}^N$ - the _recontruction bias_\n",
      "\n",
      "which encodes vectors $\\mathbf{x}$ into $\\mathbf{y} \\in [0,1]^M$ by the deterministic mapping\n",
      "\n",
      "$$ \\mathbf{h} = s(\\mathbf{x}\\mathbf{W} + \\mathbf{b}) $$\n",
      "\n",
      "Where $s$ is a poinwise sigmoidal function $s(u) = 1 / (1 + e^{-u})$.\n",
      "The latent representation $\\mathbf{h}$,\n",
      "or _code_ is then mapped back (_decoded_) into\n",
      "_reconstruction_ $\\mathbf{z}$ through the similar transformation\n",
      "\n",
      "$$\\mathbf{z} = s(\\mathbf{h}\\mathbf{V} + \\mathbf{c}) $$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run this cell to define the symbols\n",
      "def logistic(x):\n",
      "    \"\"\"Return logistic sigmoid of float or ndarray `x`\"\"\"\n",
      "    return 1.0 / (1.0 + exp(-x))\n",
      "\n",
      "def autoencoder(x, W, b, V, c):\n",
      "    h = logistic(dot(x, W) + b)\n",
      "    # -- using w.T here is called using \"tied weights\"\n",
      "    # -- using a second weight matrix here is called \"untied weights\"\n",
      "    z = logistic(dot(h, V) + c)\n",
      "    return z, h\n",
      "\n",
      "def training_criterion(x, W, b, V, c):\n",
      "    z, h = autoencoder(x, W, b, V, c)\n",
      "    reconstruction_cost = cross_entropy(x, z)\n",
      "    regularization_cost = 0 # -- we'll come back to this\n",
      "    return reconstruction_cost + regularization_cost"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Remarks on the notational conventions:\n",
      "\n",
      "* In math notation, non-bold lower-case symols denote scalars.\n",
      "\n",
      "* In math notation, bold lower-case symbols like $\\mathbf{x}$ and $\\mathbf{h}$ denote vectors.\n",
      "\n",
      "* In math notation, upper-case symbols like $W$ and $V$ typically denote matrices.\n",
      "\n",
      "* In the LaTeX-powered \"math\" expressions the notation $[a, b]$ denotes the _continuous inclusive range_ of real-valued numbers $u$ satisfying $0 \\leq x \\leq 1$.  In contrast, the Python syntax `[a, b]` denotes a _list_ data structure of two elements.\n",
      "\n",
      "* Python and NumPy tend to favor C-style \"row-major\" matrices, and we will follow that convention in the math sections too.\n",
      "Consequently, vector-matrix products will typically be written with the vector on the _left_ and the matrix on the _right_, as in $\\mathbf{h}V$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Capacity Regularization via Tied Weights\n",
      "\n",
      "One serious potential issue with auto-encoders is that if there is no other\n",
      "constraint besides minimizing the reconstruction error,\n",
      "then an auto-encoder with :math:`n` inputs and an\n",
      "encoding of dimension at least :math:`n` could potentially just learn\n",
      "the identity function, for which many encodings would be useless (e.g.,\n",
      "just copying the input), i.e., the autoencoder would not differentiate\n",
      "test examples (from the training distribution) from other input configurations.\n",
      "Surprisingly, experiments reported in [Bengio07]_ nonetheless\n",
      "suggest that in practice, when trained with\n",
      "stochastic gradient descent, non-linear auto-encoders with more hidden units\n",
      "than inputs (called overcomplete) yield useful representations\n",
      "(in the sense of classification error measured on a network taking this\n",
      "representation in input). A simple explanation is based on the\n",
      "observation that stochastic gradient\n",
      "descent with early stopping is similar to an L2 regularization of the\n",
      "parameters. To achieve perfect reconstruction of continuous\n",
      "inputs, a one-hidden layer auto-encoder with non-linear hidden units\n",
      "(exactly like in the above code)\n",
      "needs very small weights in the first (encoding) layer (to bring the non-linearity of\n",
      "the hidden units in their linear regime) and very large weights in the\n",
      "second (decoding) layer.\n",
      "With binary inputs, very large weights are\n",
      "also needed to completely minimize the reconstruction error. Since the\n",
      "implicit or explicit regularization makes it difficult to reach\n",
      "large-weight solutions, the optimization algorithm finds encodings which\n",
      "only work well for examples similar to those in the training set, which is\n",
      "what we want. It means that the representation is exploiting statistical\n",
      "regularities present in the training set, rather than learning to\n",
      "replicate the identity function.\n",
      "\n",
      "The weight matrix :math:`\\mathbf{W'}` of the reverse mapping may be\n",
      "optionally constrained by :math:`\\mathbf{W'} = \\mathbf{W}^T`, which is\n",
      "an instance of *tied weights*. The parameters of this model (namely\n",
      ":math:`\\mathbf{W}`, :math:`\\mathbf{b}`,\n",
      ":math:`\\mathbf{b'}` and, if one doesn't use tied weights, also\n",
      ":math:`\\mathbf{W'}`) are optimized such that the average reconstruction\n",
      "error is minimized. The reconstruction error can be measured in many ways, depending\n",
      "on the appropriate distributional assumptions on the input given the code, e.g., using the\n",
      "traditional *squared error* :math:`L(\\mathbf{x}, \\mathbf{z}) = || \\mathbf{x} - \\mathbf{z} ||^2`,\n",
      "or if the input is interpreted as either bit vectors or vectors of\n",
      "bit probabilities by the reconstruction *cross-entropy* defined as :\n",
      "\n",
      ".. math::\n",
      "\n",
      "  L_{H} (\\mathbf{x}, \\mathbf{z}) = - \\sum^d_{k=1}[\\mathbf{x}_k \\log\n",
      "          \\mathbf{z}_k + (1 - \\mathbf{x}_k)\\log(1 - \\mathbf{z}_k)]\n",
      "\n",
      "The hope is that the code :math:`\\mathbf{y}` is a distributed representation\n",
      "that captures the coordinates along the main factors of variation in the data\n",
      "(similarly to how the projection on principal components captures the main factors\n",
      "of variation in the data).\n",
      "Because :math:`\\mathbf{y}` is viewed as a lossy compression of :math:`\\mathbf{x}`, it cannot\n",
      "be a good compression (with small loss) for all :math:`\\mathbf{x}`, so learning\n",
      "drives it to be one that is a good compression in particular for training\n",
      "examples, and hopefully for others as well, but not for arbitrary inputs.\n",
      "That is the sense in which an auto-encoder generalizes: it gives low reconstruction\n",
      "error to test examples from the same distribution as the training examples,\n",
      "but generally high reconstruction error to uniformly chosen configurations of the\n",
      "input vector.\n",
      "\n",
      "If there is one linear hidden layer (the code) and\n",
      "the mean squared error criterion is used to train the network, then the :math:`k`\n",
      "hidden units learn to project the input in the span of the first :math:`k`\n",
      "principal components of the data. If the hidden\n",
      "layer is non-linear, the auto-encoder behaves differently from PCA,\n",
      "with the ability to capture multi-modal aspects of the input\n",
      "distribution. The departure from PCA becomes even more important when\n",
      "we consider *stacking multiple encoders* (and their corresponding decoders)\n",
      "when building a deep auto-encoder [Hinton06]_.\n",
      "\n",
      "We want to implement an auto-encoder using Theano, in the form of a class,\n",
      "that could be afterwards used in constructing a stacked autoencoder. The\n",
      "first step is to create shared variables for the parameters of the\n",
      "autoencoder ( :math:`\\mathbf{W}`, :math:`\\mathbf{b}` and\n",
      ":math:`\\mathbf{b'}`, since we are using tied weights in this tutorial ):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Capacity Regularization via Sparsity\n",
      "\n",
      "There are different ways that an auto-encoder with more hidden units\n",
      "than inputs could be prevented from learning the identity, and still\n",
      "capture something useful about the input in its hidden representation.\n",
      "One is the addition of sparsity (forcing many of the hidden units to\n",
      "be zero or near-zero), and it has been exploited very successfully\n",
      "by many [Ranzato07]_ [Lee08]_. Another is to add randomness in the transformation from\n",
      "input to reconstruction. This is exploited in Restricted Boltzmann\n",
      "Machines (discussed later in :ref:`rbm`), as well as in\n",
      "Denoising Auto-Encoders, discussed below.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions -- run this once after a kernel restart\n",
      "# Re-run it after any change you make to these routines.\n",
      "\n",
      "def euclidean_distances2(X, Y):\n",
      "    \"\"\"Return all-pairs squared distances between rows of X and Y\n",
      "    \"\"\"\n",
      "    # N.B. sklearn.metrics.pairwise.euclidean_distances\n",
      "    # offers a more robust version of this routine,\n",
      "    # but which does things that autodiff currently does not support.\n",
      "    XX = np.sum(X * X, axis=1)[:, newaxis]\n",
      "    YY = np.sum(Y * Y, axis=1)[newaxis, :]\n",
      "    distances = XX - 2 * dot(X, Y.T) + YY\n",
      "    np.maximum(distances, 0, distances)\n",
      "    return distances\n",
      "    \n",
      "\n",
      "def cross_entropy(x, x_rec_p):\n",
      "    \"\"\"Return the independent Bernoulli cross-entropy cost\n",
      "\n",
      "    x_rec_p is the Bernoulli parameter of the model's reconstruction\n",
      "    \"\"\"\n",
      "    # -- N.B. this is numerically bad, we're counting on Theano to fix up\n",
      "    return -(x * log(x_rec_p) + (1 - x) * log(1 - x_rec_p)).sum(axis=1)\n",
      "\n",
      "\n",
      "def logistic(x):\n",
      "    \"\"\"Return logistic sigmoid of float or ndarray `x`\"\"\"\n",
      "    return 1.0 / (1.0 + exp(-x))\n",
      "\n",
      "\n",
      "def softmax(x):\n",
      "    \"\"\"Return the softmax of each row in x\"\"\"\n",
      "    x2 = x - x.max(axis=1)[:, newaxis]\n",
      "    ex = exp(x2)\n",
      "    return ex / ex.sum(axis=1)[:, newaxis]\n",
      "\n",
      "\n",
      "def squared_error(x, x_rec):\n",
      "    \"\"\"Return the squared error of approximating `x` with `x_rec`\"\"\"\n",
      "    return ((x - x_rec) ** 2).sum(axis=1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pca_autoencoder_real_x(x, w, hidbias, visbias):\n",
      "    hid = dot(x - visbias, w)\n",
      "    x_rec = dot(hid, w.T)\n",
      "    cost = squared_error(x - visbias, x_rec)\n",
      "    return cost, hid\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logistic_autoencoder_binary_x(x, w, hidbias, visbias):\n",
      "    hid = logistic(dot(x, w) + hidbias)\n",
      "    # -- using w.T here is called using \"tied weights\"\n",
      "    # -- using a second weight matrix here is called \"untied weights\"\n",
      "    x_rec = logistic(dot(hid, w.T) + visbias)\n",
      "    cost = cross_entropy(x, x_rec)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def denoising_autoencoder_binary_x(x, w, hidbias, visbias, noise_level):\n",
      "    # -- corrupt the input by zero-ing out some values randomly\n",
      "    noisy_x = x * (rand(*x.shape) > noise_level)\n",
      "    hid = logistic(dot(noisy_x, w) + hidbias)\n",
      "    # -- using w.T here is called using \"tied weights\"\n",
      "    # -- using a second weight matrix here is called \"untied weights\"\n",
      "    x_rec = logistic(dot(hid, w.T) + visbias)\n",
      "    cost = cross_entropy(x, x_rec)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rbm_binary_x(x, w, hidbias, visbias):\n",
      "    hid = logistic(dot(x, w) + hidbias)\n",
      "    hid_sample = (hid > rand(*hid.shape)).astype(x.dtype)\n",
      "\n",
      "    # -- N.B. model is not actually trained to reconstruct x\n",
      "    x_rec = logistic(dot(hid_sample, w.T) + visbias)\n",
      "    x_rec_sample = (x_rec > rand(*x_rec.shape)).astype(x.dtype)\n",
      "\n",
      "    # \"negative phase\" hidden unit expectation\n",
      "    hid_rec = logistic(dot(x_rec_sample, w) + hidbias)\n",
      "\n",
      "    def free_energy(xx):\n",
      "        xw_b = dot(xx, w) + hidbias\n",
      "        return -log(1 + exp(xw_b)).sum(axis=1) - dot(xx, visbias)\n",
      "\n",
      "    cost = free_energy(x) - free_energy(x_rec_sample)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def k_means_real_x(x, w, hidbias, visbias):\n",
      "    xw = euclidean_distances2(x - visbias, w.T)\n",
      "    # -- This calculates a hard winner\n",
      "    hid = (xw == xw.min(axis=1)[:, newaxis])\n",
      "    x_rec = dot(hid, w.T)\n",
      "    cost = ((x - x_rec) ** 2).mean(axis=1)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FISTA = NotImplementedError\n",
      "# real-real Sparse Coding\n",
      "def sparse_coding_real_x(x, w, hidbias, visbias, sparse_coding_algo=FISTA):\n",
      "    # -- several sparse coding algorithms have been proposed, but they all\n",
      "    # give rise to a feature learning algorithm that looks like this:\n",
      "    hid = sparse_coding_algo(x, w)\n",
      "    x_rec = dot(hid, w.T) + visbias\n",
      "    cost = ((x - x_rec) ** 2).mean(axis=1)\n",
      "    # -- the gradient on this cost wrt `w` through the sparse_coding_algo is\n",
      "    # often ignored. At least one notable exception is the work of Karol\n",
      "    # Greggor.  I feel like the Implicit Differentiation work of Drew Bagnell\n",
      "    # is another, but I'm not sure.\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- uncomment this line to see sample images from the data set\n",
      "# show_filters(x[:100], x_img_res, (10, 10))\n",
      "\n",
      "# -- create a new model  (w, visbias, hidbias)\n",
      "w = rng.uniform(\n",
      "        low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        size=(n_visible, n_hidden)).astype(dtype)\n",
      "visbias = np.zeros(n_visible).astype(dtype)\n",
      "hidbias = np.zeros(n_hidden).astype(dtype)\n",
      "\n",
      "# show_filters(w.T, x_img_res, (n_hidden1, n_hidden2))\n",
      "x_stream = x.reshape((\n",
      "    n_examples / online_batch_size,\n",
      "    online_batch_size,\n",
      "    x.shape[1]))\n",
      "\n",
      "def train_criterion(ww, hbias, vbias, x_i=x):\n",
      "    cost, hid = algo(x_i, ww, hbias, vbias)\n",
      "    l1_cost = abs(ww).sum() * 0.0    # -- raise 0.0 to enforce l1 penalty\n",
      "    l2_cost = (ww ** 2).sum() * 0.0  # -- raise 0.0 to enforce l2 penalty\n",
      "    return cost.mean() + l1_cost + l2_cost\n",
      "\n",
      "# -- ONLINE TRAINING\n",
      "for epoch in range(online_epochs):\n",
      "    t0 = time.time()\n",
      "    w, hidbias, visbias = autodiff.fmin_sgd(train_criterion,\n",
      "            args=(w, hidbias, visbias),\n",
      "            stream=x_stream,  # -- fmin_sgd will loop through this once\n",
      "            stepsize=0.005,   # -- QQ: you should always tune this\n",
      "            print_interval=1000,\n",
      "            )\n",
      "    print 'Online training epoch %i took %f seconds' % (\n",
      "            epoch, time.time() - t0)\n",
      "    show_filters(w.T, x_img_res, (n_hidden1, n_hidden2))\n",
      "\n",
      "# -- BATCH TRAINING\n",
      "w, hidbias, visbias = autodiff.fmin_l_bfgs_b(train_criterion,\n",
      "        args=(w, hidbias, visbias),\n",
      "        # -- scipy.fmin_l_bfgs_b kwargs follow\n",
      "        maxfun=batch_epochs,\n",
      "        iprint=1,     # -- 1 for verbose, 0 for normal, -1 for quiet\n",
      "        m=lbfgs_m,         # -- how well to approximate the Hessian\n",
      "        )\n",
      "\n",
      "show_filters(w.T, x_img_res, (n_hidden1, n_hidden2)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}