{
 "metadata": {
  "name": "3_1_autoencoder"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Auto-Encoders and Denoising Auto-Encoders\n",
      "=========================================\n",
      "\n",
      "The auto-encoder (AE) is a classic unsupervised algorithm for non-linear dimensionality reduction.\n",
      "The de-noising auto-encoder (DAE) is an extension of the auto-encoder introduced as a building block for deep networks in [Vincent08].\n",
      "This tutorial introduces the autoencoder as an unsupervised feature learning algorithm for the MNIST data set, and then develops the denoising autoencoder.\n",
      "See section 4.6 of [Bengio09]_ for an overview of auto-encoders.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Before we get started with the rest of the tutorial, \n",
      "# run this once (and again after every kernel restart)\n",
      "# to bring in all the external symbols we'll need.\n",
      "\n",
      "from functools import partial\n",
      "import logging\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "from numpy import dot, exp, log, newaxis, sqrt, tanh \n",
      "from numpy.random import rand\n",
      "\n",
      "from skdata import mnist, cifar10, streetsigns\n",
      "import autodiff\n",
      "\n",
      "from utils import show_filters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's also import the MNIST data set here, so that the examples in the tutorial have some data to work with. (HINT: Some of the exercises will involve using different data sets -- you can use different data sets with the tutorial code by writing a code block like this one that redefining `x` and `x_img_res`.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtype = 'float32'   # -- this sets the working precision for data and model\n",
      "n_examples = 10000  # -- use up to 50000 examples\n",
      "\n",
      "data_view = mnist.views.OfficialImageClassification(x_dtype=dtype)\n",
      "x_as_images = data_view.train.x[:n_examples]\n",
      "x_img_res = x_as_images.shape[1:3]  # -- handy for showing filters\n",
      "x = x.reshape((n_images, -1))       # -- rasterize images to vectors\n",
      "\n",
      "# -- uncomment this line to see the initial weight values\n",
      "show_filters(x[:100], x_img_res, (10, 10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Autoencoders\n",
      "\n",
      "Generally speaking, an autoencoder maps vector input $\\mathbf{x}$ to some intermediate representation $\\mathbf{y}$ and then back into the original space, in such a way that the end-point is close to $\\mathbf{x}$. While the goal thus defined is simply to learn an identity function,\n",
      "things get interesting when the mappings are parameterized or constrained in such a way that a general identity function is either impossible to represent or at least difficult to learn from data. When this is the case, the goal of learning is to learn a _special-purpose_ identity function that _typically_ works for vectors $\\mathbf{x}$ that we care about, which come from some empirically interesting distribution.  The $\\mathbf{y}$ vector that comes out of this process contains all the _important_ information about $x$ in a new and potentially useful way.\n",
      "\n",
      "In our tutorial here we will deal with vectors $\\mathbf{x}$ that come from the MNIST data set of hand-written digits.\n",
      "Examples from MNIST are vectors $\\mathbf{x} \\in [0,1]^N$,\n",
      "and we will look at the classic one-layer sigmoidal autoencoder parameterized by:\n",
      "\n",
      "* matrix $W \\in \\mathbb{R}^{N \\times M}$ - the _weights_\n",
      "* vector $\\mathbf{b} \\in \\mathbb{R}^M$ - the _bias_\n",
      "* matrix $V \\in \\mathbb{R}^{M \\times N}$ - the _reconstruction weights_\n",
      "* vector $\\mathbf{c} \\in \\mathbb{R}^N$ - the _recontruction bias_\n",
      "\n",
      "which encodes vectors $\\mathbf{x}$ into $\\mathbf{y} \\in [0,1]^M$ by the deterministic mapping\n",
      "\n",
      "$$ \\mathbf{h} = s(\\mathbf{x}\\mathbf{W} + \\mathbf{b}) $$\n",
      "\n",
      "Where $s$ is a poinwise sigmoidal function $s(u) = 1 / (1 + e^{-u})$.\n",
      "The latent representation $\\mathbf{h}$,\n",
      "or _code_ is then mapped back (_decoded_) into\n",
      "_reconstruction_ $\\mathbf{z}$ through the similar transformation\n",
      "\n",
      "$$\\mathbf{z} = s(\\mathbf{h}\\mathbf{V} + \\mathbf{c}) $$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run this cell to define the symbols\n",
      "def logistic(u):\n",
      "    \"\"\"Return logistic sigmoid of float or ndarray `u`\"\"\"\n",
      "    return 1.0 / (1.0 + exp(-u))\n",
      "\n",
      "def autoencoder(W, b, V, c, x):\n",
      "    h = logistic(dot(x, W) + b)\n",
      "    # -- using w.T here is called using \"tied weights\"\n",
      "    # -- using a second weight matrix here is called \"untied weights\"\n",
      "    z = logistic(dot(h, V) + c)\n",
      "    return z, h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training an Autoencoder\n",
      "\n",
      "Training an autoencoder consists of minimizing a reconstruction cost,\n",
      "taking $\\mathbf{z}$ as the reconstruction of $\\mathbf{x}$.\n",
      "Intuitively, we want a model that reconstructs the important aspects of $\\mathbf{x}$\n",
      "so that we guarantee that these aspects are preserved by $\\mathbf{y}$.\n",
      "Typically, mathematical formalizations of this intuition are not particularly\n",
      "exotic, but they should, at the very least, respect the domain and range of $\\mathbf{x}$\n",
      "and $\\mathbf{z}$ respectively.\n",
      "So for example, if $\\mathbf{x}$ were a real-valued element of $\\mathbb{R}^N$ then\n",
      "we might formalize the reconstruction cost as \n",
      "$|| \\mathbf{x} - \\mathbf{z} ||^2$, but there is no mathematical requirement\n",
      "to use the $\\ell_2$ norm or indeed to use a valid norm at all.\n",
      "\n",
      "For our MNIST data, we will suppose that $\\mathbf{x}$ is a vector Bernoulli probabilities,\n",
      "and define the reconstruction cost to be the *cross-entropy* betwen $\\mathbf{z}$ and $\\mathbf{x}$:\n",
      "\n",
      "$$\n",
      "L(\\mathbf{x}, \\mathbf{z}) = - \\sum^d_{k=1}[\\mathbf{x}_k \\log\n",
      "          \\mathbf{z}_k + (1 - \\mathbf{x}_k)\\log(1 - \\mathbf{z}_k)]\n",
      "$$\n",
      "\n",
      "We write this in Python as:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cross_entropy(x, z):\n",
      "    # -- N.B. this is numerically bad, we're counting on Theano to fix up\n",
      "    return -(x * log(z) + (1 - x) * log(1 - z)).sum(axis=1)\n",
      "\n",
      "\n",
      "def training_criterion(W, b, V, c, x):\n",
      "    z, h = autoencoder(x, W, b, V, c)\n",
      "    L = cross_entropy(x, z)\n",
      "    return L"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training an autoencoder is an optimization problem that is\n",
      "typically treated in two steps:\n",
      "\n",
      "1. **Initialization** - choosing a starting point for gradient-based search,\n",
      "2. **Local Search** - running a gradient-based optimization algorithm\n",
      "\n",
      "**Initialization**\n",
      "The parameters of this particular model\n",
      "($W, \\mathbf{b}, V, \\mathbf{c}$) must be initialized\n",
      "so that symmetry is broken between the columns of $W$ (also the rows of $V$),\n",
      "otherwise the gradient on each column will be identical and local search will\n",
      "be completely ineffective.\n",
      "\n",
      "It is customary to initialize $W$ to small random values to break symmetry,\n",
      "and to initialize the rest of the parameters to 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- allocate and initialize a new model  (w, visbias, hidbias)\n",
      "\n",
      "# -- tip: choose the number of hidden units as a pair, so that show_filters works\n",
      "n_hidden2 = (10, 10)\n",
      "n_hidden = np.prod(n_hidden2)\n",
      "W = rng.uniform(\n",
      "        low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        size=(n_visible, n_hidden)).astype(dtype)\n",
      "V = np.zeros_like(W.T)\n",
      "b = np.zeros(n_visible).astype(dtype)\n",
      "c = np.zeros(n_hidden).astype(dtype)\n",
      "\n",
      "# -- uncomment this line to see the initial weight values\n",
      "show_filters(W.T, x_img_res, n_hidden2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Local Search**\n",
      "\n",
      "The training criterion $L$ defined above is a deterministic and differentiable function of parameters ($W, b, V, c$) so any multi-dimensional minimization algorithm can do the job.\n",
      "The speed and accuracy of a the method generally depends on the size nature of the data set, the parameters and parameterization of the model, and of course the sophistication of the minimization algorithm.\n",
      "\n",
      "A classic, and still widely useful, algorithm is stochastic gradient descent.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "online_batch_size = 1 # -- =1 for \"pure online\", >1 for \"minibatch\"\n",
      "\n",
      "t0 = time.time()\n",
      "W, b, V, c = autodiff.fmin_sgd(train_criterion,\n",
      "        args=(W, b, V, c),\n",
      "        streams={'x': x.reshape((\n",
      "                         n_examples / online_batch_size,\n",
      "                         online_batch_size,\n",
      "                         n_visible))}\n",
      "        stepsize=0.005,\n",
      "        loops=3,  # -- fmin_sgd can iterate over the streams repeatedly\n",
      "        print_interval=1000,\n",
      "        )\n",
      "\n",
      "show_filters(w.T, x_img_res, n_hidden2)\n",
      "print 'Online training epoch %i took %f seconds' % (\n",
      "        epoch, time.time() - t0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another powerful minimization method is the well-known (L-BFGS) algorithm. When used as a training algorithm, it is called a _batch_ algorithm because it does not deal well with stochastic functions, and therefore requires that we evaluate the total loss over all training examples on each cost function evaluation.\n",
      "\n",
      "L-BFGS can be run right away from the random initialization,\n",
      "but often what works better is to do a few loops of `fmin_sgd` to move the initial random parameters to a promising area of the search space, and then to refine that solution with L-BFGS."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- BATCH TRAINING\n",
      "def batch_criterion(W, b, V, c):\n",
      "    return train_criterion(W, b, V, c, x)\n",
      "\n",
      "W, b, V, c = autodiff.fmin_l_bfgs_b(batch_criterion,\n",
      "        args=(W, b, V, c),\n",
      "        maxfun=20,    # -- how many function calls to allow?\n",
      "        iprint=1,     # -- 1 for verbose, 0 for normal, -1 for quiet\n",
      "        m=20,         # -- how well to approximate the Hessian\n",
      "        )\n",
      "\n",
      "show_filters(w.T, x_img_res, n_hid2) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's it, we've trained an auto-encoder.\n",
      "\n",
      "### Autoencoder Summary\n",
      "\n",
      "An autoencoder is a function that maps a vector to approximately itself, by passing it through intermediate values in such a way that the pure identify function is difficult or impossible.\n",
      "An autoencoder is a non-stochastic pre-cursur to several more modern probabilistic models such as De-Noising AutoEncoders, Sparse Coding, and Restricted Boltzmann Machines. They are a flexible class of feature extraction algorithms that can be quick to train by either stochastic gradient descent, or more sophisticated minimization algorithms such as L-BFGS.\n",
      "\n",
      "XXXX PUT LINKS TO OTHER NOTEBOOKS ABOVE\n",
      "\n",
      "Spend some time running through the exercises below to get a better feel for how autoencoders work and what they can do.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Remarks on the notational conventions:\n",
      "\n",
      "XXX MOVE THIS TO THE FIRST TUTORIAL IT APPLIES TO ALL OF THEM.\n",
      "\n",
      "* Expressions that appear in fixed-width font like `a = b + c` are meant to be interpreted as Python code, whereas expressions appearing in italics like $a = b + c$ are meant to be interpreted as math notation.\n",
      "\n",
      "* In math notation, non-bold lower-case symols denote scalars.\n",
      "\n",
      "* In math notation, bold lower-case symbols like $\\mathbf{x}$ and $\\mathbf{h}$ denote vectors.\n",
      "\n",
      "* In math notation, upper-case symbols like $W$ and $V$ typically denote matrices.\n",
      "\n",
      "* In the LaTeX-powered \"math\" expressions the notation $[a, b]$ denotes the _continuous inclusive range_ of real-valued numbers $u$ satisfying $0 \\leq x \\leq 1$.  In contrast, the Python syntax `[a, b]` denotes a _list_ data structure of two elements.\n",
      "\n",
      "* Python and NumPy tend to favor C-style \"row-major\" matrices, and we will follow that convention in the math sections too.\n",
      "Consequently, vector-matrix products will typically be written with the vector on the _left_ and the matrix on the _right_, as in $\\mathbf{h}V$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Linear AutoEncoder\n",
      "\n",
      "If there is one linear hidden layer (the code) and\n",
      "the mean squared error criterion is used to train the network, then the :math:`k`\n",
      "hidden units learn to project the input in the span of the first :math:`k`\n",
      "principal components of the data. If the hidden\n",
      "layer is non-linear, the auto-encoder behaves differently from PCA,\n",
      "with the ability to capture multi-modal aspects of the input\n",
      "distribution. The departure from PCA becomes even more important when\n",
      "we consider *stacking multiple encoders* (and their corresponding decoders)\n",
      "when building a deep auto-encoder [Hinton06]_.\n",
      "\n",
      "XXX "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Capacity Regularization via Tied Weights\n",
      "\n",
      "One serious potential issue with auto-encoders is that if there is no other\n",
      "constraint besides minimizing the reconstruction error,\n",
      "then an auto-encoder with :math:`n` inputs and an\n",
      "encoding of dimension at least :math:`n` could potentially just learn\n",
      "the identity function, for which many encodings would be useless (e.g.,\n",
      "just copying the input), i.e., the autoencoder would not differentiate\n",
      "test examples (from the training distribution) from other input configurations.\n",
      "Surprisingly, experiments reported in [Bengio07]_ nonetheless\n",
      "suggest that in practice, when trained with\n",
      "stochastic gradient descent, non-linear auto-encoders with more hidden units\n",
      "than inputs (called overcomplete) yield useful representations\n",
      "(in the sense of classification error measured on a network taking this\n",
      "representation in input). A simple explanation is based on the\n",
      "observation that stochastic gradient\n",
      "descent with early stopping is similar to an L2 regularization of the\n",
      "parameters. To achieve perfect reconstruction of continuous\n",
      "inputs, a one-hidden layer auto-encoder with non-linear hidden units\n",
      "(exactly like in the above code)\n",
      "needs very small weights in the first (encoding) layer (to bring the non-linearity of\n",
      "the hidden units in their linear regime) and very large weights in the\n",
      "second (decoding) layer.\n",
      "With binary inputs, very large weights are\n",
      "also needed to completely minimize the reconstruction error. Since the\n",
      "implicit or explicit regularization makes it difficult to reach\n",
      "large-weight solutions, the optimization algorithm finds encodings which\n",
      "only work well for examples similar to those in the training set, which is\n",
      "what we want. It means that the representation is exploiting statistical\n",
      "regularities present in the training set, rather than learning to\n",
      "replicate the identity function.\n",
      "\n",
      "The weight matrix :math:`\\mathbf{W'}` of the reverse mapping may be\n",
      "optionally constrained by :math:`\\mathbf{W'} = \\mathbf{W}^T`, which is\n",
      "an instance of *tied weights*. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Capacity Regularization via Sparsity\n",
      "\n",
      "There are different ways that an auto-encoder with more hidden units\n",
      "than inputs could be prevented from learning the identity, and still\n",
      "capture something useful about the input in its hidden representation.\n",
      "One is the addition of sparsity (forcing many of the hidden units to\n",
      "be zero or near-zero), and it has been exploited very successfully\n",
      "by many [Ranzato07]_ [Lee08]_. Another is to add randomness in the transformation from\n",
      "input to reconstruction. This is exploited in Restricted Boltzmann\n",
      "Machines (discussed later in :ref:`rbm`), as well as in\n",
      "Denoising Auto-Encoders, discussed below.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions -- run this once after a kernel restart\n",
      "# Re-run it after any change you make to these routines.\n",
      "\n",
      "def euclidean_distances2(X, Y):\n",
      "    \"\"\"Return all-pairs squared distances between rows of X and Y\n",
      "    \"\"\"\n",
      "    # N.B. sklearn.metrics.pairwise.euclidean_distances\n",
      "    # offers a more robust version of this routine,\n",
      "    # but which does things that autodiff currently does not support.\n",
      "    XX = np.sum(X * X, axis=1)[:, newaxis]\n",
      "    YY = np.sum(Y * Y, axis=1)[newaxis, :]\n",
      "    distances = XX - 2 * dot(X, Y.T) + YY\n",
      "    np.maximum(distances, 0, distances)\n",
      "    return distances\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logistic(x):\n",
      "    \"\"\"Return logistic sigmoid of float or ndarray `x`\"\"\"\n",
      "    return 1.0 / (1.0 + exp(-x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def softmax(x):\n",
      "    \"\"\"Return the softmax of each row in x\"\"\"\n",
      "    x2 = x - x.max(axis=1)[:, newaxis]\n",
      "    ex = exp(x2)\n",
      "    return ex / ex.sum(axis=1)[:, newaxis]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def squared_error(x, x_rec):\n",
      "    \"\"\"Return the squared error of approximating `x` with `x_rec`\"\"\"\n",
      "    return ((x - x_rec) ** 2).sum(axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pca_autoencoder_real_x(x, w, hidbias, visbias):\n",
      "    hid = dot(x - visbias, w)\n",
      "    x_rec = dot(hid, w.T)\n",
      "    cost = squared_error(x - visbias, x_rec)\n",
      "    return cost, hid\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logistic_autoencoder_binary_x(x, w, hidbias, visbias):\n",
      "    hid = logistic(dot(x, w) + hidbias)\n",
      "    # -- using w.T here is called using \"tied weights\"\n",
      "    # -- using a second weight matrix here is called \"untied weights\"\n",
      "    x_rec = logistic(dot(hid, w.T) + visbias)\n",
      "    cost = cross_entropy(x, x_rec)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def denoising_autoencoder_binary_x(x, w, hidbias, visbias, noise_level):\n",
      "    # -- corrupt the input by zero-ing out some values randomly\n",
      "    noisy_x = x * (rand(*x.shape) > noise_level)\n",
      "    hid = logistic(dot(noisy_x, w) + hidbias)\n",
      "    # -- using w.T here is called using \"tied weights\"\n",
      "    # -- using a second weight matrix here is called \"untied weights\"\n",
      "    x_rec = logistic(dot(hid, w.T) + visbias)\n",
      "    cost = cross_entropy(x, x_rec)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rbm_binary_x(x, w, hidbias, visbias):\n",
      "    hid = logistic(dot(x, w) + hidbias)\n",
      "    hid_sample = (hid > rand(*hid.shape)).astype(x.dtype)\n",
      "\n",
      "    # -- N.B. model is not actually trained to reconstruct x\n",
      "    x_rec = logistic(dot(hid_sample, w.T) + visbias)\n",
      "    x_rec_sample = (x_rec > rand(*x_rec.shape)).astype(x.dtype)\n",
      "\n",
      "    # \"negative phase\" hidden unit expectation\n",
      "    hid_rec = logistic(dot(x_rec_sample, w) + hidbias)\n",
      "\n",
      "    def free_energy(xx):\n",
      "        xw_b = dot(xx, w) + hidbias\n",
      "        return -log(1 + exp(xw_b)).sum(axis=1) - dot(xx, visbias)\n",
      "\n",
      "    cost = free_energy(x) - free_energy(x_rec_sample)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def k_means_real_x(x, w, hidbias, visbias):\n",
      "    xw = euclidean_distances2(x - visbias, w.T)\n",
      "    # -- This calculates a hard winner\n",
      "    hid = (xw == xw.min(axis=1)[:, newaxis])\n",
      "    x_rec = dot(hid, w.T)\n",
      "    cost = ((x - x_rec) ** 2).mean(axis=1)\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FISTA = NotImplementedError\n",
      "# real-real Sparse Coding\n",
      "def sparse_coding_real_x(x, w, hidbias, visbias, sparse_coding_algo=FISTA):\n",
      "    # -- several sparse coding algorithms have been proposed, but they all\n",
      "    # give rise to a feature learning algorithm that looks like this:\n",
      "    hid = sparse_coding_algo(x, w)\n",
      "    x_rec = dot(hid, w.T) + visbias\n",
      "    cost = ((x - x_rec) ** 2).mean(axis=1)\n",
      "    # -- the gradient on this cost wrt `w` through the sparse_coding_algo is\n",
      "    # often ignored. At least one notable exception is the work of Karol\n",
      "    # Greggor.  I feel like the Implicit Differentiation work of Drew Bagnell\n",
      "    # is another, but I'm not sure.\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    }
   ],
   "metadata": {}
  }
 ]
}