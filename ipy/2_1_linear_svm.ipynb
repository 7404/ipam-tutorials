{
 "metadata": {
  "name": "2_1_linear_svm"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear Support Vector Machine\n",
      "=============================\n",
      "\n",
      "The linear support vector machine (SVM) is a linear classifier parametrized by a matrix of weights $W$ and a bias vector $\\mathbf{b}$.  We will develop the multiclass \"One vs. All\" linear support vector machine, which is a concatentation of two-class support vector machines. We will suppose that our label set is the non-negative integers up to some maximum L."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize the workspace by importing several symbols\n",
      "import logging\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "from numpy import arange, dot, maximum, ones, tanh, zeros\n",
      "from numpy.random import randn\n",
      "\n",
      "from skdata import mnist\n",
      "import autodiff\n",
      "\n",
      "from util import show_filters\n",
      "#from utils import show_filters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- load and prepare the data set (even download if necessary)\n",
      "dtype = 'float32'\n",
      "n_examples = 10000\n",
      "n_classes = 10         # -- denoted L in the math expressions\n",
      "img_shape = (28, 28)\n",
      "\n",
      "data_view = mnist.views.OfficialVectorClassification(x_dtype=dtype)\n",
      "x = data_view.train.x[:n_examples]\n",
      "y = data_view.train.y[:n_examples]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supposing that our input is a vector (generally a _feature vector_) $\\mathbf{x}$, the prediction $l^*$ of the OVA-SVM is the argmax of the affine function:\n",
      "\n",
      "$$\n",
      "l^* = \\operatorname*{argmax}_{l=0}^{L} (\\mathbf{x} W_l + \\mathbf{b}_l)\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ova_svm_prediction(W, b, x):\n",
      "    return np.argmax(np.dot(x, W) + b, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training an SVM\n",
      "\n",
      "The most standard way of training an SVM is to maximize the _margin_ on training data.\n",
      "The _margin_ is always defined in the two-class case (where $y \\in \\\\{-1, +1\\\\}$) \n",
      "as $y * (xW + b)$.\n",
      "The affine function parametrized by $W$ and $b$ defines a hyper-plane where $\\mathbf{x}W + b = 0$ that the SVM interprets as a decision surface.\n",
      "The _margin_ represents how far away $\\mathbf{x}W+b$ is from being on the wrong side of the decision surface: large positive values mean there is a safe distance, negative values mean that the decision surface is actually incorrect for the given $\\mathbf{x}$.\n",
      "\n",
      "The most standard sense in which SVMs maximize margin is via the _hinge loss_. The hinge loss of margin value $u$ is defined as\n",
      "\n",
      "$$\n",
      "\\mathrm{hinge}(u) = \\max(0, 1-u)\n",
      "$$\n",
      "\n",
      "By maximizing the average hinge loss of the margin of training examples, we maximize the tightest convex upper bound on the mis-classification rate (zero-one loss), and can \n",
      "get a good binary classifier using fast algorithms for convex optimization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hinge(u):\n",
      "    return np.maximum(0, 1 - u)\n",
      "\n",
      "ugrid = np.arange(-5, 5, .1)\n",
      "plot(ugrid, hinge(ugrid), label='hinge loss')\n",
      "plot(ugrid, ugrid < 0, label='zero-one loss')\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the multiclass case, it is not clear what the \"correct\" margin definition should be, and several useful ones have been proposed.\n",
      "For the rest of this tutorial we'll develop the \"one vs. all\" multiclass SVM, sometimes called an OVA-SVM.\n",
      "\n",
      "In the OVA-SVM, the training objective is defined by $L$ independent SVMs.\n",
      "We will train an OVA-SVM by converting the integer-valued labels\n",
      "$y$ to $Y \\in \\\\{-1, +1\\\\}^{N \\times L}$ and training $L$ SVMs at once.\n",
      "The $L$ columns of $Y$ represent binary classification tasks.\n",
      "The $L$ columns of $W$ and $L$ elements of $\\mathbf{b}$ store the parameters of the $L$ SVMs.\n",
      "\n",
      "The [unregularized] training objective is:\n",
      "\n",
      "$$\n",
      "\\mathcal{L}(\\mathcal{D}; W, \\mathbf{b}) =\n",
      "\\frac{1}{N}\n",
      "\\sum_{(\\mathbf{x}^{(i)}, Y^{(i)}) \\in \\mathcal{D}}\n",
      "~\n",
      "\\sum_{l=1}^{L}\n",
      "~\n",
      "\\max \\left( 0, 1 - Y^{(i)}_l (\\mathbf{x}^{(i)} W_l + \\mathbf{b}_l) \\right)\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- prepare a \"1-hot\" version of the labels, denoted Y in the math\n",
      "y1 = -1 * ones((len(y), n_classes)).astype(dtype)\n",
      "y1[arange(len(y)), y] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ova_svm_cost(W, b, x, y1):\n",
      "    # -- one vs. all linear SVM loss\n",
      "    margin = y1 * (dot(x, W) + b)\n",
      "    cost = hinge(margin).mean(axis=0).sum()\n",
      "    return cost\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The training itself consists in minimizing the objective $\\mathcal{L}(\\mathcal{D}; W, b)$ with respect to $W$ and $b$.  The criterion is convex, so it doesn't much matter where we start. Zero works well in practice."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize the model\n",
      "W = zeros((x.shape[1], n_classes), dtype=dtype)\n",
      "b = zeros(n_classes, dtype=dtype)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many specialized SVM solvers available, but they tend to be most helpful for non-linear SVMs.  In the linear case a simple combination of stochastic gradient descent (SGD) and BFGS can be very effective."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- do n_online_loops passes through the data set doing SGD\n",
      "#    This is a good pre-conditioning process prior to L-BFGS\n",
      "t0 = time.time()\n",
      "online_batch_size = 1\n",
      "n_online_epochs = 1\n",
      "n_batches = n_examples / online_batch_size\n",
      "W, b = autodiff.fmin_sgd(ova_svm_cost, (W, b),\n",
      "            streams={\n",
      "                'x': x.reshape((n_batches, online_batch_size, x.shape[1])),\n",
      "                'y1': y1.reshape((n_batches, online_batch_size, y1.shape[1]))},\n",
      "            loops=n_online_epochs,\n",
      "            stepsize=0.001,\n",
      "            print_interval=1000,\n",
      "            )\n",
      "print 'SGD took %.2f seconds' % (time.time() - t0)\n",
      "show_filters(W.T, img_shape, (2, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- L-BFGS optimization of our SVM cost.\n",
      "def batch_criterion(W, b):\n",
      "    return ova_svm_cost(W, b, x, y1)\n",
      "W, b = autodiff.fmin_l_bfgs_b(batch_criterion, (W, b), maxfun=20, m=20, iprint=1)\n",
      "#   -- the output from this command comes from Fortran, so iPython does not see it.\n",
      "#      To monitor progress, look at the terminal from which you launched ipython\n",
      "show_filters(W.T, img_shape, (2, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing the SVM\n",
      "\n",
      "Once the a classifier has been trained, we can test it for generalization accuracy on the test set. We test it by making predictions for the examples in the test set and counting up the number of classification mistakes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_predictions = ova_svm_prediction(W, b, x)\n",
      "train_errors = y != train_predictions\n",
      "print 'Current train set error rate', np.mean(train_errors)\n",
      "\n",
      "test_predictions = ova_svm_prediction(W, b, data_view.test.x[:])\n",
      "test_errors = data_view.test.y[:] != test_predictions\n",
      "print 'Current test set error rate', np.mean(test_errors)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: L1 and L2 regularization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Different Data Sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How are the pixel colors encoded in each data set? How are the labels encoded?  Tip: the labels are in `train.y`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Ranking SVM\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}